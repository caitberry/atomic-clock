{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404757a0",
   "metadata": {},
   "source": [
    "# Select data day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "622dc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math \n",
    "from typing import List, Union, Optional\n",
    "from astropy.time import Time\n",
    "import missingno as msno\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def open_ErYb_data(data_path):\n",
    "    key2read = [\"MJD\", \"timer\", \"SDR:frep_ErYb\", \"fo_ErYb\", \"fb_Si_ErYb\", \"fb_Yb_ErYb\", \"fb_Al_ErYb\"] \n",
    "    types = {key: str for key in key2read}\n",
    "    types[\"MJD\"] = float\n",
    " \n",
    "    data = pd.read_csv(data_path, header=1, delimiter=\"\\t\", dtype=types, engine=\"python\")\n",
    " \n",
    "    for k in key2read:\n",
    "        data[k] = data[k].apply(Decimal)\n",
    " \n",
    "    data.index = range(len(data))\n",
    " \n",
    "    return data[list(types.keys())]\n",
    "\n",
    "def open_shiftfile_Al(datapath):\n",
    "    data = pd.read_csv(datapath, header=30, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    "\n",
    "def open_shiftfile_Sr(datapath): ##Note: assuming (unverified) that for days_irregular all Sr .dat files have this format.... \n",
    "    data = pd.read_csv(datapath, header=24, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    "\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    "def open_shiftfile_Yb(datapath):\n",
    "    data = pd.read_csv(datapath, header=8, delimiter=r\"\\t\",  dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    "def open_maser_correction(datapath):\n",
    "    data = pd.read_csv(datapath, header=1, delimiter=\",\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    data.columns = [\"date\", \"maser_offset\"]\n",
    " \n",
    "    data[\"date\"] = data[\"date\"].apply(str)#.str.split(\"-\").str.join(\"\")\n",
    "    data[\"maser_offset\"] = data[\"maser_offset\"].apply(float)\n",
    " \n",
    "    return data\n",
    "\n",
    "days = [20250116, 20250124, 20250204, 20250227, 20250304, 20250307, 20250318]\n",
    "days = list(map(str, days))\n",
    "days_irregular = [20250206, 20250228, 20250306, 20250313, 20250320, 20250321]\n",
    "days_irregular = list(map(str, days_irregular))\n",
    "## ? YbSr ratio for all 13 days \n",
    "## ? AlYb and AlSr ratio only for 9 of the 13 days \n",
    "day_index = 0\n",
    "path = \"/Users/smt3/Documents/GitHub/2025 clock comparison data/\"\n",
    "day_irregular_index = None\n",
    "## For days_irregular, read in Sr data with the following \n",
    "if day_irregular_index != None:\n",
    "    if days_irregular[day_irregular_index] == 20250206:\n",
    "        shift_data_Sr = pd.concat(\n",
    "            [\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock0.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock2.dat\"),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    elif days_irregular[day_irregular_index] == 20250228:\n",
    "        shift_data_Sr = open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock1.dat\")\n",
    "    elif days_irregular[day_irregular_index] == 20250306:\n",
    "        shift_data_Sr = open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock2.dat\")\n",
    "    elif days_irregular[day_irregular_index] == 20250313:\n",
    "        shift_data_Sr = pd.concat(\n",
    "            [\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock0.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock1.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock2.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock3.dat\"),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    elif days_irregular[day_irregular_index] == 20250320:\n",
    "        shift_data_Sr = pd.concat(\n",
    "            [\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock0.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock2.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock4.dat\"),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    elif days_irregular[day_irregular_index] == 20250321:\n",
    "        shift_data_Sr = pd.concat(\n",
    "            [\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock0.dat\"),\n",
    "                open_shiftfile_Sr(path + days_irregular[day_irregular_index] + \"/\" + days_irregular[day_irregular_index] + \"_clock_lock1.dat\"),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "else: \n",
    "    shift_data_Sr = open_shiftfile_Sr(path + days[day_index] + \"/\" + days[day_index] + \"_clock_lock0.dat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652ed54",
   "metadata": {},
   "source": [
    "# Read in data and corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19655975",
   "metadata": {},
   "outputs": [],
   "source": [
    "maser_corrections = open_maser_correction(path + \"daily maser offsets.csv\")\n",
    "data_ErYb = open_ErYb_data(path + days[day_index] + \"/\" + days[day_index] + \"_Deglitched_ErYb_only.dat\") \n",
    "shift_data_Al = open_shiftfile_Al(path + days[day_index] + \"/\" + days[day_index] + \"_Alp_Freq_Shifts_ErYb.dat\")\n",
    "shift_data_Yb = open_shiftfile_Yb(path + days[day_index] + \"/YbI_1_rerun.txt\")\n",
    "\n",
    "def compute_nuAl_ErYb(data):\n",
    "    data[\"nuAl\"] = -Decimal(\"105e6\") + Decimal(\"560444\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Al_ErYb\"]\n",
    "    data[\"nuAl\"] = Decimal(4) * data[\"nuAl\"]   \n",
    "\n",
    "def compute_nuSr_ErYb(data):\n",
    "    data[\"nuSi\"] = -Decimal(\"105e6\") + Decimal(\"388752\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - Decimal(\"100e6\")\n",
    "    data[\"nuSr\"] = (Decimal(\"1716882\") / Decimal(\"777577\")) * (data[\"nuSi\"] - Decimal(\"216e6\"))\n",
    "\n",
    "def compute_nuYb_ErYb(data):\n",
    "    data[\"nuYb\"] = -Decimal(\"105e6\") + Decimal(\"518237\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Yb_ErYb\"]\n",
    "    data[\"nuYb\"] = Decimal(2) * data[\"nuYb\"] \n",
    "\n",
    "compute_nuAl_ErYb(data_ErYb)\n",
    "compute_nuSr_ErYb(data_ErYb)\n",
    "compute_nuYb_ErYb(data_ErYb) \n",
    "\n",
    "YbSrRatio2020 = Decimal(\"1.2075070393433378482\") \n",
    "AlYbRatio2020 = Decimal(\"2.162887127516663703\")\n",
    "AlSrRatio2020 = Decimal(\"2.611701431781463025\")\n",
    " \n",
    "correction_condition = days[day_index] == maser_corrections[\"date\"]\n",
    "masercorrection = maser_corrections[correction_condition][\"maser_offset\"].apply(Decimal)\n",
    "\n",
    "GR_shift_Al = Decimal(\"-8.114e-16\") \n",
    "GR_shift_Yb = Decimal(\"-8.109e-16\")\n",
    "GR_shift_Sr = Decimal(\"10.660e-16\")\n",
    "GR_shift_sea_level = Decimal(\"-1798.501e-16\")\n",
    "\n",
    "total_correction_Yb = Decimal(\"1\") + GR_shift_Yb + GR_shift_sea_level + masercorrection\n",
    "total_correction_Sr = Decimal(\"1\") + GR_shift_Sr + GR_shift_sea_level + masercorrection\n",
    "total_correction_Al = Decimal(\"1\") + GR_shift_Al + GR_shift_sea_level + masercorrection\n",
    "# print(total_correction_Yb)\n",
    "# print(total_correction_Sr)\n",
    "# print(total_correction_Al)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26513bf3",
   "metadata": {},
   "source": [
    "# Use only good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f76cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_cond = ~shift_data_Al['shift'].isna()\n",
    "Al_non_na = shift_data_Al[al_cond]\n",
    "Al = pd.Series(Al_non_na['MJD'])\n",
    "\n",
    "sr_cond = ~shift_data_Sr['shift'].isna()\n",
    "Sr_non_na = shift_data_Sr[sr_cond]\n",
    "Sr = pd.Series(Sr_non_na['MJD'])\n",
    "\n",
    "yb_cond = ~shift_data_Yb['shift'].isna()\n",
    "Yb_non_na = shift_data_Yb[yb_cond]\n",
    "Yb = pd.Series(Yb_non_na['MJD']) \n",
    "\n",
    "comb_condition = (~data_ErYb['nuAl'].isna() & ~data_ErYb['nuSr'].isna() & ~data_ErYb['nuYb'].isna())\n",
    "comb_full = data_ErYb[comb_condition]\n",
    "\n",
    "good_condition_al = Al_non_na[\"IS_GOOD\"] == 1\n",
    "shift_data_Al_good = Al_non_na[good_condition_al].reset_index(drop=True, inplace = False)\n",
    "Al_good = pd.Series(shift_data_Al_good['MJD'])\n",
    "\n",
    "good_condition_sr = Sr_non_na[\"IS_GOOD\"] == 1\n",
    "shift_data_Sr_good = Sr_non_na[good_condition_sr].reset_index(drop=True, inplace = False)\n",
    "Sr_good = pd.Series(shift_data_Sr_good['MJD'])\n",
    "\n",
    "good_condition_yb = Yb_non_na[\"IS_GOOD\"] == 1\n",
    "shift_data_Yb_good = Yb_non_na[good_condition_yb].reset_index(drop=True, inplace = False)\n",
    "Yb_good = pd.Series(shift_data_Yb_good['MJD']) \n",
    "\n",
    "len_comb = len(comb_full['MJD']) \n",
    "len_Al = len(shift_data_Al_good['shift'])        \n",
    "len_Sr = len(shift_data_Sr_good['shift'])        \n",
    "len_Yb = len(shift_data_Yb_good['shift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266907f",
   "metadata": {},
   "source": [
    "# Overlapping window of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c901a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comb start and end MJD: [ 60691.915175 ,  60692.059256 ]\n",
      "Al good shift start and end MJD: [ 60692.0118056 ,  60692.0553819 ]\n",
      "Sr good shift start and end MJD: [ 60691.77047008579 ,  60692.055575832455 ]\n",
      "Yb good shift start and end MJD: [ 60691.90146037 ,  60692.05947302 ]\n",
      "Last start time:  60692.0118056\n",
      "First end time:  60692.0553819\n",
      "nuAl, nuSr, and nuYb start and end MJD: [ 60692.011800 ,  60692.055375 ]\n",
      "Al good shift start and end MJD: [ 60692.0118056 ,  60692.0553704 ]\n",
      "Sr good shift start and end MJD: [ 60692.01176064856 ,  60692.0553617726 ]\n",
      "Yb good shift start and end MJD: [ 60692.01180446 ,  60692.05537606 ]\n"
     ]
    }
   ],
   "source": [
    "##TODO: generalize this to look for paired windows (rather than one window amongst all three)\n",
    "print(\"Comb start and end MJD: [\", '{:0.11}'.format(comb_full['MJD'].iloc[0]), ', ', '{:0.11}'.format(comb_full['MJD'].iloc[len_comb-1]), ']')\n",
    "print(\"Al good shift start and end MJD: [\", shift_data_Al_good['MJD'].iloc[0], ', ', shift_data_Al_good['MJD'].iloc[len_Al-1], ']')\n",
    "print(\"Sr good shift start and end MJD: [\", shift_data_Sr_good['MJD'].iloc[0], ', ', shift_data_Sr_good['MJD'].iloc[len_Sr-1], ']')\n",
    "print(\"Yb good shift start and end MJD: [\", shift_data_Yb_good['MJD'].iloc[0], ', ', shift_data_Yb_good['MJD'].iloc[len_Yb-1], ']')\n",
    "\n",
    "starts = [comb_full['MJD'].iloc[0], shift_data_Al_good['MJD'].iloc[0], shift_data_Sr_good['MJD'].iloc[0], shift_data_Yb_good['MJD'].iloc[0]] \n",
    "ends = [comb_full['MJD'].iloc[len_comb-1], shift_data_Al_good['MJD'].iloc[len_Al-1], shift_data_Sr_good['MJD'].iloc[len_Sr-1], shift_data_Yb_good['MJD'].iloc[len_Yb-1]] \n",
    "\n",
    "last_start_time = max(starts)\n",
    "first_end_time = min(ends)\n",
    "\n",
    "print(\"Last start time: \", last_start_time)\n",
    "print(\"First end time: \", first_end_time)\n",
    "\n",
    "def lb_extract(target, data):\n",
    "    inx = 0\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data.iloc[inx] < target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return inx  \n",
    "\n",
    "def ub_extract(target, data):\n",
    "    inx = 1\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data.iloc[len(data)-inx] > target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return len(data)-inx \n",
    "\n",
    "comb_start = ub_extract(target = last_start_time, data = comb_full['MJD'])  \n",
    "comb_end = lb_extract(target = first_end_time, data = comb_full['MJD']) \n",
    "\n",
    "comb = pd.DataFrame()\n",
    "comb[\"MJD\"] = comb_full['MJD'].iloc[comb_start:comb_end] \n",
    "comb[\"nuAl\"] = comb_full['nuAl'].iloc[comb_start:comb_end]\n",
    "comb[\"nuSr\"] = comb_full['nuSr'].iloc[comb_start:comb_end]\n",
    "comb[\"nuYb\"] = comb_full['nuYb'].iloc[comb_start:comb_end]\n",
    "comb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "al_start = ub_extract(target = last_start_time, data = shift_data_Al_good[\"MJD\"])\n",
    "al_end = lb_extract(target = first_end_time, data = shift_data_Al_good[\"MJD\"])  \n",
    "shift_data_Al = shift_data_Al_good[al_start:al_end] \n",
    "shift_data_Al.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sr_start = ub_extract(target = last_start_time, data = shift_data_Sr_good[\"MJD\"])\n",
    "sr_end = lb_extract(target = first_end_time, data = shift_data_Sr_good[\"MJD\"])  \n",
    "shift_data_Sr = shift_data_Sr_good[sr_start:sr_end]\n",
    "shift_data_Sr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "yb_start = ub_extract(target = last_start_time, data = shift_data_Yb_good[\"MJD\"])\n",
    "yb_end = lb_extract(target = first_end_time, data = shift_data_Yb_good[\"MJD\"])  \n",
    "shift_data_Yb = shift_data_Yb_good[yb_start:yb_end]\n",
    "shift_data_Yb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"nuAl, nuSr, and nuYb start and end MJD: [\", '{:0.11}'.format(comb[\"MJD\"].iloc[0]), ', ', '{:0.11}'.format(comb[\"MJD\"].iloc[len(comb[\"MJD\"])-1]), ']')\n",
    "print(\"Al good shift start and end MJD: [\", shift_data_Al['MJD'].iloc[0], ', ', shift_data_Al['MJD'].iloc[len(shift_data_Al['MJD'])-1], ']')\n",
    "print(\"Sr good shift start and end MJD: [\", shift_data_Sr['MJD'].iloc[0], ', ', shift_data_Sr['MJD'].iloc[len(shift_data_Sr['MJD'])-1], ']')\n",
    "print(\"Yb good shift start and end MJD: [\", shift_data_Yb['MJD'].iloc[0], ', ', shift_data_Yb['MJD'].iloc[len(shift_data_Yb['MJD'])-1], ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4647dd9",
   "metadata": {},
   "source": [
    "# Create datetime index for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71a7f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_datetime = comb.copy()\n",
    "comb_datetime['datetime'] = Time(comb_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "comb_datetime = comb_datetime.set_index('datetime')\n",
    "\n",
    "shift_data_Al_datetime = shift_data_Al.copy()\n",
    "shift_data_Al_datetime['datetime'] = Time(shift_data_Al_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "shift_data_Al_datetime = shift_data_Al_datetime.set_index('datetime')\n",
    "\n",
    "shift_data_Sr_datetime = shift_data_Sr.copy()\n",
    "shift_data_Sr_datetime['datetime'] = Time(shift_data_Sr_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "shift_data_Sr_datetime = shift_data_Sr_datetime.set_index('datetime')\n",
    "\n",
    "shift_data_Yb_datetime = shift_data_Yb.copy()\n",
    "shift_data_Yb_datetime['datetime'] = Time(shift_data_Yb_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "shift_data_Yb_datetime = shift_data_Yb_datetime.set_index('datetime')\n",
    "\n",
    "Al_shift = shift_data_Al_datetime['shift']\n",
    "Sr_shift = shift_data_Sr_datetime['shift']\n",
    "Yb_shift = shift_data_Yb_datetime['shift']\n",
    "\n",
    "interp_times_Al = comb_datetime.index.difference(Al_shift.index) \n",
    "long_Al_index = Al_shift.index.union(interp_times_Al).sort_values()\n",
    "Al_shift_expanded = Al_shift.reindex(long_Al_index)\n",
    "\n",
    "interp_times_Sr = comb_datetime.index.difference(Sr_shift.index) \n",
    "long_Sr_index = Sr_shift.index.union(interp_times_Sr).sort_values()\n",
    "Sr_shift_expanded = Sr_shift.reindex(long_Sr_index)\n",
    "\n",
    "interp_times_Yb = comb_datetime.index.difference(Yb_shift.index) \n",
    "long_Yb_index = Yb_shift.index.union(interp_times_Yb).sort_values()\n",
    "Yb_shift_expanded = Yb_shift.reindex(long_Yb_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae52284",
   "metadata": {},
   "source": [
    "# Necessary supplemental functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a5df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_avar_fn(y, m): #for computing AVAR from data with missing values \n",
    "    M = len(y)\n",
    "\n",
    "    if M < 2 * m:\n",
    "        raise ValueError(f\"Length of input (M={M}) must be at least 2 * m (2 * {m} = {2 * m})\")\n",
    "\n",
    "    if any(isinstance(v, Decimal) and v.is_nan() for v in y):  \n",
    "        raise ValueError(\"Input y contains NaN values.\")\n",
    "    \n",
    "    if m <= 0:\n",
    "        raise ValueError(\"m must be a positive integer\")\n",
    "\n",
    "    outer_sum = 0\n",
    "\n",
    "    for j in range(0, M - 2 * m + 1):\n",
    "        inner_sum = 0\n",
    "        for i in range(j, j + m):\n",
    "            inner_sum += y[i + m] - y[i]\n",
    "        outer_sum += inner_sum ** 2\n",
    "\n",
    "    result = outer_sum / (2 * m**2 * (M - 2 * m + 1))\n",
    "    return result\n",
    "\n",
    "def clean_frequency_ratio(frequency_ratio_data: List[Optional[Union[float, Decimal]]]) -> List[Union[float, Decimal]]:\n",
    "    return [\n",
    "        x for x in frequency_ratio_data\n",
    "        if x is not None\n",
    "        and not (\n",
    "            (isinstance(x, float) and math.isnan(x)) or\n",
    "            (isinstance(x, Decimal) and x.is_nan())\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def detect_long_missing(data, max_len): #for kalman smoothing interpolation limit \n",
    "    mask = data.isna()\n",
    "    long_gaps = pd.Series(False, index=data.index)\n",
    "\n",
    "    start = None\n",
    "    for i, val in enumerate(mask):\n",
    "        if val and start is None:\n",
    "            start = i\n",
    "        elif not val and start is not None:\n",
    "            if i - start > max_len:\n",
    "                long_gaps[start:i] = True\n",
    "            start = None\n",
    "    if start is not None and len(data) - start > max_len:\n",
    "        long_gaps[start:] = True\n",
    "\n",
    "    return long_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc0a71",
   "metadata": {},
   "source": [
    "# Interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d626925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smt3/Documents/myvenv/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/smt3/Documents/myvenv/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Users/smt3/Documents/myvenv/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/var/folders/hb/crd62ysx4f1745w1jxnlrch8002d5c/T/ipykernel_217/3281443039.py:50: FutureWarning: Series.interpolate with method=pad is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  Al_shift_interpolated = Al_shift_expanded.interpolate(method=my_method, limit=interp_limit)\n",
      "/var/folders/hb/crd62ysx4f1745w1jxnlrch8002d5c/T/ipykernel_217/3281443039.py:56: FutureWarning: Series.interpolate with method=pad is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  Sr_shift_interpolated = Sr_shift_expanded.interpolate(method=my_method, limit=interp_limit)\n",
      "/var/folders/hb/crd62ysx4f1745w1jxnlrch8002d5c/T/ipykernel_217/3281443039.py:62: FutureWarning: Series.interpolate with method=pad is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  Yb_shift_interpolated = Yb_shift_expanded.interpolate(method=my_method, limit=interp_limit)\n"
     ]
    }
   ],
   "source": [
    "## Create a dictionary to store all six interpolation results \n",
    "interp_Al_results = {} \n",
    "interp_Sr_results = {} \n",
    "interp_Yb_results = {} \n",
    "interp_methods = ['time', 'linear', 'pad', 'nearest', 'cubic', 'kalman']  \n",
    "interp_limit = 10\n",
    "\n",
    "#Kalman setup \n",
    "Al_model = sm.tsa.UnobservedComponents(Al_shift_expanded, level='local level')\n",
    "Sr_model = sm.tsa.UnobservedComponents(Sr_shift_expanded, level='local level')\n",
    "Yb_model = sm.tsa.UnobservedComponents(Yb_shift_expanded, level='local level')\n",
    "\n",
    "Al_result = Al_model.fit(method='lbfgs') \n",
    "Sr_result = Sr_model.fit(method='lbfgs')\n",
    "Yb_result = Yb_model.fit(method='lbfgs')\n",
    "\n",
    "Al_smoothed_level = Al_result.smoothed_state[0]\n",
    "Sr_smoothed_level = Sr_result.smoothed_state[0]\n",
    "Yb_smoothed_level = Yb_result.smoothed_state[0]  \n",
    "\n",
    "for my_method in interp_methods:\n",
    "    if my_method == \"kalman\":\n",
    "        ## kalman specific \n",
    "        Al_shift_interpolated = pd.Series(Al_smoothed_level, index=Al_shift_expanded.index)\n",
    "        Sr_shift_interpolated = pd.Series(Sr_smoothed_level, index=Sr_shift_expanded.index)\n",
    "        Yb_shift_interpolated = pd.Series(Yb_smoothed_level, index=Yb_shift_expanded.index)\n",
    "\n",
    "        Al_long_gap_mask = detect_long_missing(Al_shift_expanded, max_len=interp_limit)\n",
    "        Al_shift_interpolated[Al_long_gap_mask] = np.nan\n",
    "        Al_shift_final = Al_shift_interpolated[comb_datetime.index]\n",
    "        Al_key_name = f\"Al_shift_final_{my_method}\"\n",
    "        interp_Al_results[Al_key_name] = Al_shift_final.copy()\n",
    "        interp_Al_results[Al_key_name].name = Al_key_name\n",
    "\n",
    "        Sr_long_gap_mask = detect_long_missing(Sr_shift_expanded, max_len=interp_limit)\n",
    "        Sr_shift_interpolated[Sr_long_gap_mask] = np.nan\n",
    "        Sr_shift_final = Sr_shift_interpolated[comb_datetime.index]\n",
    "        Sr_key_name = f\"Sr_shift_final_{my_method}\"\n",
    "        interp_Sr_results[Sr_key_name] = Sr_shift_final.copy()\n",
    "        interp_Sr_results[Sr_key_name].name = Sr_key_name\n",
    "\n",
    "        Yb_long_gap_mask = detect_long_missing(Yb_shift_expanded, max_len=interp_limit)\n",
    "        Yb_shift_interpolated[Yb_long_gap_mask] = np.nan\n",
    "        Yb_shift_final = Yb_shift_interpolated[comb_datetime.index]\n",
    "        Yb_key_name = f\"Yb_shift_final_{my_method}\"\n",
    "        interp_Yb_results[Yb_key_name] = Yb_shift_final.copy()\n",
    "        interp_Yb_results[Yb_key_name].name = Yb_key_name\n",
    "    else: \n",
    "        # general interpolation \n",
    "        Al_shift_interpolated = Al_shift_expanded.interpolate(method=my_method, limit=interp_limit)\n",
    "        Al_shift_final = Al_shift_interpolated[comb_datetime.index]\n",
    "        Al_key_name = f\"Al_shift_final_{my_method}\"\n",
    "        interp_Al_results[Al_key_name] = Al_shift_final.copy()\n",
    "        interp_Al_results[Al_key_name].name = Al_key_name\n",
    "\n",
    "        Sr_shift_interpolated = Sr_shift_expanded.interpolate(method=my_method, limit=interp_limit)\n",
    "        Sr_shift_final = Sr_shift_interpolated[comb_datetime.index]\n",
    "        Sr_key_name = f\"Sr_shift_final_{my_method}\"\n",
    "        interp_Sr_results[Sr_key_name] = Sr_shift_final.copy()\n",
    "        interp_Sr_results[Sr_key_name].name = Sr_key_name\n",
    "\n",
    "        Yb_shift_interpolated = Yb_shift_expanded.interpolate(method=my_method, limit=interp_limit)\n",
    "        Yb_shift_final = Yb_shift_interpolated[comb_datetime.index]\n",
    "        Yb_key_name = f\"Yb_shift_final_{my_method}\"\n",
    "        interp_Yb_results[Yb_key_name] = Yb_shift_final.copy()\n",
    "        interp_Yb_results[Yb_key_name].name = Yb_key_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec91de7",
   "metadata": {},
   "source": [
    "## Analyze results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ef11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  20250116  Method:  kalman \n",
      "\n",
      "Al+/Sr ratio offset from BACON paper -1.3570E-16\n",
      "Yb/Sr ratio offset from BACON paper -1.0030E-16\n",
      "Al+/Yb ratio offset from BACON paper -3.4786E-17 \n",
      "\n",
      "Al+/Sr ADEV with tau= 1130 :  4.3210E-18\n",
      "Yb/Sr ADEV with tau= 1130 :  4.3210E-18\n",
      "Al+/Yb ADEV with tau= 1178 :  6.1869E-18\n"
     ]
    }
   ],
   "source": [
    "##specify which interpolation results to view of ['time', 'linear', 'pad', 'nearest', 'cubic', 'kalman'] \n",
    "method_res = \"kalman\" \n",
    "\n",
    "\n",
    "if method_res == \"kalman\":\n",
    "    starting_index = 30\n",
    "else:\n",
    "    starting_index = 0\n",
    "    \n",
    "## process results for analysis \n",
    "nuAl = [Decimal(i) for i in comb['nuAl']]\n",
    "nuSr = [Decimal(i) for i in comb['nuSr']]\n",
    "nuYb = [Decimal(i) for i in comb['nuYb']]\n",
    "\n",
    "shiftAl = [Decimal(i) for i in interp_Al_results[f\"Al_shift_final_{method_res}\"]]\n",
    "shiftSr = [Decimal(i) for i in interp_Sr_results[f\"Sr_shift_final_{method_res}\"]]\n",
    "shiftYb = [Decimal(i) for i in interp_Yb_results[f\"Yb_shift_final_{method_res}\"]]\n",
    "\n",
    "frequency_Al_ErYb = [((i + j) * total_correction_Al).iloc[0] for i, j in zip(nuAl[starting_index:], shiftAl[starting_index:])]\n",
    "frequency_Sr_ErYb = [((i + j) * total_correction_Sr).iloc[0] for i, j in zip(nuSr[starting_index:], shiftSr[starting_index:])]\n",
    "frequency_Yb_ErYb = [((i + j) * total_correction_Yb).iloc[0] for i, j in zip(nuYb[starting_index:], shiftYb[starting_index:])]\n",
    "\n",
    "frequency_ratio_ErYb1 = [(i / j - AlSrRatio2020)/AlSrRatio2020 for i,j in zip(frequency_Al_ErYb, frequency_Sr_ErYb)]\n",
    "frequency_ratio_ErYb2 = [(i / j - YbSrRatio2020)/YbSrRatio2020 for i,j in zip(frequency_Yb_ErYb, frequency_Sr_ErYb)]\n",
    "frequency_ratio_ErYb3 = [(i / j - AlYbRatio2020)/AlYbRatio2020 for i,j in zip(frequency_Al_ErYb, frequency_Yb_ErYb)]\n",
    "\n",
    "clean_frequency_ratio_ErYb1 = clean_frequency_ratio(frequency_ratio_ErYb1)\n",
    "clean_frequency_ratio_ErYb2 = clean_frequency_ratio(frequency_ratio_ErYb2)\n",
    "clean_frequency_ratio_ErYb3 = clean_frequency_ratio(frequency_ratio_ErYb3)\n",
    "\n",
    "## print summary statistics \n",
    "if day_irregular_index != None:\n",
    "    print(\"Date: \", days_irregular[day_irregular_index], \" Method: \", method_res, \"\\n\")\n",
    "else: \n",
    "    print(\"Date: \", days[day_index], \" Method: \", method_res, \"\\n\")\n",
    "print(\"Al+/Sr ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb1)))\n",
    "print(\"Yb/Sr ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb2)))\n",
    "print(\"Al+/Yb ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb3)), '\\n')\n",
    "\n",
    "print(\"Al+/Sr ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb1)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb1, math.floor(len(clean_frequency_ratio_ErYb1)/3)).sqrt()))\n",
    "print(\"Yb/Sr ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb2)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb1, math.floor(len(clean_frequency_ratio_ErYb2)/3)).sqrt())) \n",
    "print(\"Al+/Yb ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb3)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb3, math.floor(len(clean_frequency_ratio_ErYb1)/3)).sqrt()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
