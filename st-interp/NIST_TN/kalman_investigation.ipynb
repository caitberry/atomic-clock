{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404757a0",
   "metadata": {},
   "source": [
    "# Select data day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622dc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math \n",
    "from typing import List, Union, Optional\n",
    "from astropy.time import Time\n",
    "import missingno as msno\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def open_ErYb_data(data_path):\n",
    "    key2read = [\"MJD\", \"timer\", \"SDR:frep_ErYb\", \"fo_ErYb\", \"fb_Si_ErYb\", \"fb_Yb_ErYb\", \"fb_Al_ErYb\"] \n",
    "    types = {key: str for key in key2read}\n",
    "    types[\"MJD\"] = float\n",
    " \n",
    "    data = pd.read_csv(data_path, header=1, delimiter=\"\\t\", dtype=types, engine=\"python\")\n",
    " \n",
    "    for k in key2read:\n",
    "        data[k] = data[k].apply(Decimal)\n",
    " \n",
    "    data.index = range(len(data))\n",
    " \n",
    "    return data[list(types.keys())]\n",
    "\n",
    "def open_shiftfile_Al(datapath):\n",
    "    data = pd.read_csv(datapath, header=30, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    "\n",
    "def open_shiftfile_Sr(datapath):\n",
    "    data = pd.read_csv(datapath, header=24, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    "\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    "def open_shiftfile_Yb(datapath):\n",
    "    data = pd.read_csv(datapath, header=8, delimiter=r\"\\t\",  dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    "def open_maser_correction(datapath):\n",
    "    data = pd.read_csv(datapath, header=1, delimiter=\",\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    data.columns = [\"date\", \"maser_offset\"]\n",
    " \n",
    "    data[\"date\"] = data[\"date\"].str.split(\"-\").str.join(\"\")\n",
    "    data[\"maser_offset\"] = data[\"maser_offset\"].apply(float)\n",
    " \n",
    "    return data\n",
    "\n",
    "days = [20250116, 20250124, 20250204, 20250227, 20250304, 20250307, 20250318]\n",
    "days = list(map(str, days))\n",
    "days_irregular = [20250206, 20250228, 20250306, 20250313, 20250320, 20250321]\n",
    "days_irregular = list(map(str, days_irregular))\n",
    "##TODO: figure out how to read in days_irregular data \n",
    "day_index = 0\n",
    "path = \"/Users/smt3/Documents/GitHub/2025 clock comparison data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652ed54",
   "metadata": {},
   "source": [
    "# Read in data and corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19655975",
   "metadata": {},
   "outputs": [],
   "source": [
    "maser_corrections = open_maser_correction(path + \"daily maser offsets.csv\")\n",
    "data_ErYb = open_ErYb_data(path + days[day_index] + \"/\" + days[day_index] + \"_Deglitched_ErYb_only.dat\") \n",
    "shift_data_Al = open_shiftfile_Al(path + days[day_index] + \"/\" + days[day_index] + \"_Alp_Freq_Shifts_ErYb.dat\")\n",
    "shift_data_Sr = open_shiftfile_Sr(path + days[day_index] + \"/\" + days[day_index] + \"_clock_lock0.dat\")\n",
    "shift_data_Yb = open_shiftfile_Yb(path + days[day_index] + \"/YbI_1_rerun.txt\")\n",
    "\n",
    "def compute_nuAl_ErYb(data):\n",
    "    data[\"nuAl\"] = -Decimal(\"105e6\") + Decimal(\"560444\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Al_ErYb\"]\n",
    "    data[\"nuAl\"] = Decimal(4) * data[\"nuAl\"]   \n",
    "\n",
    "def compute_nuSr_ErYb(data):\n",
    "    data[\"nuSi\"] = -Decimal(\"105e6\") + Decimal(\"388752\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - Decimal(\"100e6\")\n",
    "    data[\"nuSr\"] = (Decimal(\"1716882\") / Decimal(\"777577\")) * (data[\"nuSi\"] - Decimal(\"216e6\"))\n",
    "\n",
    "def compute_nuYb_ErYb(data):\n",
    "    data[\"nuYb\"] = -Decimal(\"105e6\") + Decimal(\"518237\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Yb_ErYb\"]\n",
    "    data[\"nuYb\"] = Decimal(2) * data[\"nuYb\"] \n",
    "\n",
    "compute_nuAl_ErYb(data_ErYb)\n",
    "compute_nuSr_ErYb(data_ErYb)\n",
    "compute_nuYb_ErYb(data_ErYb) \n",
    "\n",
    "YbSrRatio2020 = Decimal(\"1.2075070393433378482\") \n",
    "AlYbRatio2020 = Decimal(\"2.162887127516663703\")\n",
    "AlSrRatio2020 = Decimal(\"2.611701431781463025\")\n",
    " \n",
    "correction_condition = days[day_index] == maser_corrections[\"date\"]\n",
    "masercorrection = maser_corrections[correction_condition][\"maser_offset\"].apply(Decimal)\n",
    "\n",
    "GR_shift_Al = Decimal(\"-8.114e-16\") \n",
    "GR_shift_Yb = Decimal(\"-8.109e-16\")\n",
    "GR_shift_Sr = Decimal(\"10.660e-16\")\n",
    "GR_shift_sea_level = Decimal(\"-1798.501e-16\")\n",
    "\n",
    "total_correction_Yb = Decimal(\"1\") + GR_shift_Yb + GR_shift_sea_level + masercorrection\n",
    "total_correction_Sr = Decimal(\"1\") + GR_shift_Sr + GR_shift_sea_level + masercorrection\n",
    "total_correction_Al = Decimal(\"1\") + GR_shift_Al + GR_shift_sea_level + masercorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26513bf3",
   "metadata": {},
   "source": [
    "# Use only good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_cond = ~shift_data_Al['shift'].isna()\n",
    "Al_non_na = shift_data_Al[al_cond]\n",
    "Al = pd.Series(Al_non_na['MJD'])\n",
    "\n",
    "sr_cond = ~shift_data_Sr['shift'].isna()\n",
    "Sr_non_na = shift_data_Sr[sr_cond]\n",
    "Sr = pd.Series(Sr_non_na['MJD'])\n",
    "\n",
    "yb_cond = ~shift_data_Yb['shift'].isna()\n",
    "Yb_non_na = shift_data_Yb[yb_cond]\n",
    "Yb = pd.Series(Yb_non_na['MJD']) \n",
    "\n",
    "comb_condition = (~data_ErYb['nuAl'].isna() & ~data_ErYb['nuSr'].isna() & ~data_ErYb['nuYb'].isna())\n",
    "comb_full = data_ErYb[comb_condition]\n",
    "\n",
    "good_condition_al = Al_non_na[\"IS_GOOD\"] == 1\n",
    "shift_data_Al_good = Al_non_na[good_condition_al].reset_index(drop=True, inplace = False)\n",
    "Al_good = pd.Series(shift_data_Al_good['MJD'])\n",
    "\n",
    "good_condition_sr = Sr_non_na[\"IS_GOOD\"] == 1\n",
    "shift_data_Sr_good = Sr_non_na[good_condition_sr].reset_index(drop=True, inplace = False)\n",
    "Sr_good = pd.Series(shift_data_Sr_good['MJD'])\n",
    "\n",
    "good_condition_yb = Yb_non_na[\"IS_GOOD\"] == 1\n",
    "shift_data_Yb_good = Yb_non_na[good_condition_yb].reset_index(drop=True, inplace = False)\n",
    "Yb_good = pd.Series(shift_data_Yb_good['MJD']) \n",
    "\n",
    "len_comb = len(comb_full['MJD']) \n",
    "len_Al = len(shift_data_Al_good['shift'])        \n",
    "len_Sr = len(shift_data_Sr_good['shift'])        \n",
    "len_Yb = len(shift_data_Yb_good['shift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266907f",
   "metadata": {},
   "source": [
    "# Overlapping window of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comb start and end MJD: [\", '{:0.11}'.format(comb_full['MJD'].iloc[0]), ', ', '{:0.11}'.format(comb_full['MJD'].iloc[len_comb-1]), ']')\n",
    "print(\"Al good shift start and end MJD: [\", shift_data_Al_good['MJD'].iloc[0], ', ', shift_data_Al_good['MJD'].iloc[len_Al-1], ']')\n",
    "print(\"Sr good shift start and end MJD: [\", shift_data_Sr_good['MJD'].iloc[0], ', ', shift_data_Sr_good['MJD'].iloc[len_Sr-1], ']')\n",
    "print(\"Yb good shift start and end MJD: [\", shift_data_Yb_good['MJD'].iloc[0], ', ', shift_data_Yb_good['MJD'].iloc[len_Yb-1], ']')\n",
    "\n",
    "starts = [comb_full['MJD'].iloc[0], shift_data_Al_good['MJD'].iloc[0], shift_data_Sr_good['MJD'].iloc[0], shift_data_Yb_good['MJD'].iloc[0]] \n",
    "ends = [comb_full['MJD'].iloc[len_comb-1], shift_data_Al_good['MJD'].iloc[len_Al-1], shift_data_Sr_good['MJD'].iloc[len_Sr-1], shift_data_Yb_good['MJD'].iloc[len_Yb-1]] \n",
    "\n",
    "last_start_time = max(starts)\n",
    "first_end_time = min(ends)\n",
    "\n",
    "print(\"Last start time: \", last_start_time)\n",
    "print(\"First end time: \", first_end_time)\n",
    "\n",
    "def lb_extract(target, data):\n",
    "    inx = 0\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data.iloc[inx] < target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return inx  \n",
    "\n",
    "def ub_extract(target, data):\n",
    "    inx = 1\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data.iloc[len(data)-inx] > target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return len(data)-inx \n",
    "\n",
    "comb_start = ub_extract(target = last_start_time, data = comb_full['MJD'])  \n",
    "comb_end = lb_extract(target = first_end_time, data = comb_full['MJD']) \n",
    "\n",
    "comb = pd.DataFrame()\n",
    "comb[\"MJD\"] = comb_full['MJD'].iloc[comb_start:comb_end] \n",
    "comb[\"nuAl\"] = comb_full['nuAl'].iloc[comb_start:comb_end]\n",
    "comb[\"nuSr\"] = comb_full['nuSr'].iloc[comb_start:comb_end]\n",
    "comb[\"nuYb\"] = comb_full['nuYb'].iloc[comb_start:comb_end]\n",
    "comb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "al_start = ub_extract(target = last_start_time, data = shift_data_Al_good[\"MJD\"])\n",
    "al_end = lb_extract(target = first_end_time, data = shift_data_Al_good[\"MJD\"])  \n",
    "shift_data_Al = shift_data_Al_good[al_start:al_end] \n",
    "shift_data_Al.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sr_start = ub_extract(target = last_start_time, data = shift_data_Sr_good[\"MJD\"])\n",
    "sr_end = lb_extract(target = first_end_time, data = shift_data_Sr_good[\"MJD\"])  \n",
    "shift_data_Sr = shift_data_Sr_good[sr_start:sr_end]\n",
    "shift_data_Sr.reset_index(drop=True, inplace=True)\n",
    "\n",
    "yb_start = ub_extract(target = last_start_time, data = shift_data_Yb_good[\"MJD\"])\n",
    "yb_end = lb_extract(target = first_end_time, data = shift_data_Yb_good[\"MJD\"])  \n",
    "shift_data_Yb = shift_data_Yb_good[yb_start:yb_end]\n",
    "shift_data_Yb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"nuAl, nuSr, and nuYb start and end MJD: [\", '{:0.11}'.format(comb[\"MJD\"].iloc[0]), ', ', '{:0.11}'.format(comb[\"MJD\"].iloc[len(comb[\"MJD\"])-1]), ']')\n",
    "print(\"Al good shift start and end MJD: [\", shift_data_Al['MJD'].iloc[0], ', ', shift_data_Al['MJD'].iloc[len(shift_data_Al['MJD'])-1], ']')\n",
    "print(\"Sr good shift start and end MJD: [\", shift_data_Sr['MJD'].iloc[0], ', ', shift_data_Sr['MJD'].iloc[len(shift_data_Sr['MJD'])-1], ']')\n",
    "print(\"Yb good shift start and end MJD: [\", shift_data_Yb['MJD'].iloc[0], ', ', shift_data_Yb['MJD'].iloc[len(shift_data_Yb['MJD'])-1], ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4647dd9",
   "metadata": {},
   "source": [
    "# Create datetime index for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_datetime = comb.copy()\n",
    "comb_datetime['datetime'] = Time(comb_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "comb_datetime = comb_datetime.set_index('datetime')\n",
    "\n",
    "shift_data_Al_datetime = shift_data_Al.copy()\n",
    "shift_data_Al_datetime['datetime'] = Time(shift_data_Al_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "shift_data_Al_datetime = shift_data_Al_datetime.set_index('datetime')\n",
    "\n",
    "shift_data_Sr_datetime = shift_data_Sr.copy()\n",
    "shift_data_Sr_datetime['datetime'] = Time(shift_data_Sr_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "shift_data_Sr_datetime = shift_data_Sr_datetime.set_index('datetime')\n",
    "\n",
    "shift_data_Yb_datetime = shift_data_Yb.copy()\n",
    "shift_data_Yb_datetime['datetime'] = Time(shift_data_Yb_datetime['MJD'], format = 'mjd').to_datetime()\n",
    "shift_data_Yb_datetime = shift_data_Yb_datetime.set_index('datetime')\n",
    "\n",
    "Al_shift = shift_data_Al_datetime['shift']\n",
    "Sr_shift = shift_data_Sr_datetime['shift']\n",
    "Yb_shift = shift_data_Yb_datetime['shift']\n",
    "\n",
    "interp_times_Al = comb_datetime.index.difference(Al_shift.index) \n",
    "long_Al_index = Al_shift.index.union(interp_times_Al).sort_values()\n",
    "Al_shift_expanded = Al_shift.reindex(long_Al_index)\n",
    "\n",
    "interp_times_Sr = comb_datetime.index.difference(Sr_shift.index) \n",
    "long_Sr_index = Sr_shift.index.union(interp_times_Sr).sort_values()\n",
    "Sr_shift_expanded = Sr_shift.reindex(long_Sr_index)\n",
    "\n",
    "interp_times_Yb = comb_datetime.index.difference(Yb_shift.index) \n",
    "long_Yb_index = Yb_shift.index.union(interp_times_Yb).sort_values()\n",
    "Yb_shift_expanded = Yb_shift.reindex(long_Yb_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae52284",
   "metadata": {},
   "source": [
    "# Functions necessary for computing overlapping AVAR from data with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5df233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_avar_fn(y, m): \n",
    "    M = len(y)\n",
    "\n",
    "    if M < 2 * m:\n",
    "        raise ValueError(f\"Length of input (M={M}) must be at least 2 * m (2 * {m} = {2 * m})\")\n",
    "\n",
    "    if any(isinstance(v, Decimal) and v.is_nan() for v in y):  \n",
    "        raise ValueError(\"Input y contains NaN values.\")\n",
    "    \n",
    "    if m <= 0:\n",
    "        raise ValueError(\"m must be a positive integer\")\n",
    "\n",
    "    outer_sum = 0\n",
    "\n",
    "    for j in range(0, M - 2 * m + 1):\n",
    "        inner_sum = 0\n",
    "        for i in range(j, j + m):\n",
    "            inner_sum += y[i + m] - y[i]\n",
    "        outer_sum += inner_sum ** 2\n",
    "\n",
    "    result = outer_sum / (2 * m**2 * (M - 2 * m + 1))\n",
    "    return result\n",
    "\n",
    "def clean_frequency_ratio(frequency_ratio_data: List[Optional[Union[float, Decimal]]]) -> List[Union[float, Decimal]]:\n",
    "    return [\n",
    "        x for x in frequency_ratio_data\n",
    "        if x is not None\n",
    "        and not (\n",
    "            (isinstance(x, float) and math.isnan(x)) or\n",
    "            (isinstance(x, Decimal) and x.is_nan())\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc0a71",
   "metadata": {},
   "source": [
    "# Time-based linear interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d626925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: unique varb names for each method: time, linear, pad, nearest, cubic, kalman\n",
    "interp_method = 'time' \n",
    "interp_limit = 10\n",
    "\n",
    "Al_shift_interpolated = Al_shift_expanded.interpolate(method=interp_method, limit=interp_limit)\n",
    "Al_shift_final = Al_shift_interpolated[comb_datetime.index]\n",
    "\n",
    "Sr_shift_interpolated = Sr_shift_expanded.interpolate(method=interp_method, limit=interp_limit)\n",
    "Sr_shift_final = Sr_shift_interpolated[comb_datetime.index]\n",
    "\n",
    "Yb_shift_interpolated = Yb_shift_expanded.interpolate(method=interp_method, limit=interp_limit)\n",
    "Yb_shift_final = Yb_shift_interpolated[comb_datetime.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec91de7",
   "metadata": {},
   "source": [
    "## Mean and ADEV for time-based interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuAl = [Decimal(i) for i in comb['nuAl']]\n",
    "nuSr = [Decimal(i) for i in comb['nuSr']]\n",
    "nuYb = [Decimal(i) for i in comb['nuYb']]\n",
    "\n",
    "shiftAl = [Decimal(i) for i in Al_shift_final]\n",
    "shiftSr = [Decimal(i) for i in Sr_shift_final]\n",
    "shiftYb = [Decimal(i) for i in Yb_shift_final]\n",
    "\n",
    "frequency_Al_ErYb = [((i + j) * total_correction_Al).iloc[0] for i, j in zip(nuAl, shiftAl)]\n",
    "frequency_Sr_ErYb = [((i + j) * total_correction_Sr).iloc[0] for i, j in zip(nuSr, shiftSr)]\n",
    "frequency_Yb_ErYb = [((i + j) * total_correction_Yb).iloc[0] for i, j in zip(nuYb, shiftYb)]\n",
    "\n",
    "frequency_ratio_ErYb1 = [(i / j - AlSrRatio2020)/AlSrRatio2020 for i,j in zip(frequency_Al_ErYb, frequency_Sr_ErYb)]\n",
    "frequency_ratio_ErYb2 = [(i / j - YbSrRatio2020)/YbSrRatio2020 for i,j in zip(frequency_Yb_ErYb, frequency_Sr_ErYb)]\n",
    "frequency_ratio_ErYb3 = [(i / j - AlYbRatio2020)/AlYbRatio2020 for i,j in zip(frequency_Al_ErYb, frequency_Yb_ErYb)]\n",
    "\n",
    "clean_frequency_ratio_ErYb1 = clean_frequency_ratio(frequency_ratio_ErYb1)\n",
    "clean_frequency_ratio_ErYb2 = clean_frequency_ratio(frequency_ratio_ErYb2)\n",
    "clean_frequency_ratio_ErYb3 = clean_frequency_ratio(frequency_ratio_ErYb3)\n",
    "\n",
    "print(\"Date: \", days[day_index], \" Method: Time\", \"\\n\")\n",
    "print(\"Al+/Sr ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb1)))\n",
    "print(\"Yb/Sr ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb2)))\n",
    "print(\"Al+/Yb ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb3)))\n",
    "\n",
    "print(\"Al+/Sr ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb1)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb1, math.floor(len(clean_frequency_ratio_ErYb1)/3)).sqrt()))\n",
    "print(\"Yb/Sr ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb2)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb1, math.floor(len(clean_frequency_ratio_ErYb2)/3)).sqrt())) \n",
    "print(\"Al+/Yb ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb3)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb3, math.floor(len(clean_frequency_ratio_ErYb1)/3)).sqrt()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d37ad",
   "metadata": {},
   "source": [
    "# Kalman smoothing interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b043747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_long_missing(data, max_len=10):\n",
    "    mask = data.isna()\n",
    "    long_gaps = pd.Series(False, index=data.index)\n",
    "\n",
    "    start = None\n",
    "    for i, val in enumerate(mask):\n",
    "        if val and start is None:\n",
    "            start = i\n",
    "        elif not val and start is not None:\n",
    "            if i - start > max_len:\n",
    "                long_gaps[start:i] = True\n",
    "            start = None\n",
    "    if start is not None and len(data) - start > max_len:\n",
    "        long_gaps[start:] = True\n",
    "\n",
    "    return long_gaps\n",
    "\n",
    "Al_model = sm.tsa.UnobservedComponents(Al_shift_expanded, level='local level')\n",
    "Sr_model = sm.tsa.UnobservedComponents(Sr_shift_expanded, level='local level')\n",
    "Yb_model = sm.tsa.UnobservedComponents(Yb_shift_expanded, level='local level')\n",
    "\n",
    "Al_result = Al_model.fit(method='lbfgs') \n",
    "Sr_result = Sr_model.fit(method='lbfgs')\n",
    "Yb_result = Yb_model.fit(method='lbfgs')\n",
    "\n",
    "Al_smoothed_level = Al_result.smoothed_state[0]\n",
    "Sr_smoothed_level = Sr_result.smoothed_state[0]\n",
    "Yb_smoothed_level = Yb_result.smoothed_state[0]  \n",
    "\n",
    "Al_shift_interpolated = pd.Series(Al_smoothed_level, index=Al_shift_expanded.index)\n",
    "Sr_shift_interpolated = pd.Series(Sr_smoothed_level, index=Sr_shift_expanded.index)\n",
    "Yb_shift_interpolated = pd.Series(Yb_smoothed_level, index=Yb_shift_expanded.index)\n",
    "\n",
    "Al_long_gap_mask = detect_long_missing(Al_shift_expanded, max_len=10)\n",
    "Al_shift_interpolated[Al_long_gap_mask] = np.nan\n",
    "Al_shift_final = Al_shift_interpolated[comb_datetime.index]\n",
    "\n",
    "Sr_long_gap_mask = detect_long_missing(Sr_shift_expanded, max_len=10)\n",
    "Sr_shift_interpolated[Sr_long_gap_mask] = np.nan\n",
    "Sr_shift_final = Sr_shift_interpolated[comb_datetime.index]\n",
    "\n",
    "Yb_long_gap_mask = detect_long_missing(Yb_shift_expanded, max_len=10)\n",
    "Yb_shift_interpolated[Yb_long_gap_mask] = np.nan\n",
    "Yb_shift_final = Yb_shift_interpolated[comb_datetime.index]\n",
    "\n",
    "nuAl = [Decimal(i) for i in comb['nuAl']]\n",
    "nuSr = [Decimal(i) for i in comb['nuSr']]\n",
    "nuYb = [Decimal(i) for i in comb['nuYb']]\n",
    "\n",
    "shiftAl = [Decimal(i) for i in Al_shift_final]\n",
    "shiftSr = [Decimal(i) for i in Sr_shift_final]\n",
    "shiftYb = [Decimal(i) for i in Yb_shift_final]\n",
    "\n",
    "starting_index = 30 #choose how many initial points to exclude \n",
    "\n",
    "frequency_Al_ErYb_KF = [((i + j) * total_correction_Al).iloc[0] for i, j in zip(nuAl[starting_index:], shiftAl[starting_index:])]\n",
    "frequency_Sr_ErYb_KF = [((i + j) * total_correction_Sr).iloc[0] for i, j in zip(nuSr[starting_index:], shiftSr[starting_index:])]\n",
    "frequency_Yb_ErYb_KF = [((i + j) * total_correction_Yb).iloc[0] for i, j in zip(nuYb[starting_index:], shiftYb[starting_index:])]\n",
    "\n",
    "frequency_ratio_ErYb1_KF = [(i / j - AlSrRatio2020)/AlSrRatio2020 for i,j in zip(frequency_Al_ErYb_KF, frequency_Sr_ErYb_KF)]\n",
    "frequency_ratio_ErYb2_KF = [(i / j - YbSrRatio2020)/YbSrRatio2020 for i,j in zip(frequency_Yb_ErYb_KF, frequency_Sr_ErYb_KF)]\n",
    "frequency_ratio_ErYb3_KF = [(i / j - AlYbRatio2020)/AlYbRatio2020 for i,j in zip(frequency_Al_ErYb_KF, frequency_Yb_ErYb_KF)]\n",
    "\n",
    "clean_frequency_ratio_ErYb1_KF = clean_frequency_ratio(frequency_ratio_ErYb1_KF)\n",
    "clean_frequency_ratio_ErYb2_KF = clean_frequency_ratio(frequency_ratio_ErYb2_KF)\n",
    "clean_frequency_ratio_ErYb3_KF = clean_frequency_ratio(frequency_ratio_ErYb3_KF)\n",
    "\n",
    "print(\"Date: \", days[day_index], \" Method: Kalman\", \"\\n\")\n",
    "print(\"Al+/Sr ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb1_KF)))\n",
    "print(\"Yb/Sr ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb2_KF)))\n",
    "print(\"Al+/Yb ratio offset from BACON paper\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb3_KF)), '\\n')\n",
    "\n",
    "print(\"Al+/Sr ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb1_KF)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb1_KF, math.floor(len(clean_frequency_ratio_ErYb1_KF)/3)).sqrt()))\n",
    "print(\"Yb/Sr ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb2_KF)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb1_KF, math.floor(len(clean_frequency_ratio_ErYb2_KF)/3)).sqrt())) \n",
    "print(\"Al+/Yb ADEV with tau=\", math.floor(len(clean_frequency_ratio_ErYb3_KF)/3), \": \", '{:0.5}'.format(overlapping_avar_fn(clean_frequency_ratio_ErYb3_KF, math.floor(len(clean_frequency_ratio_ErYb1_KF)/3)).sqrt()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
