---
title: "Clock data interpolation TN draft"
author: "Thornton"
output:
  word_document:
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(MASS)
```


<!--- Open Command Palette: Press F1 or Ctrl + Shift + P to open the command palette.
Run Knit Command: Type rmarkdown: Render and select the desired output format (e.g., HTML, PDF).--->
## Introduction

The purpose of this report is to present a methodological approach for applying imputation and interpolation techniques to clock frequency data. This report is a result of collaborative research into statistical methods for the analysis of high-precision atomic clock data by the Statistical Engineering Division of the Information Technology Laboratory and the Time and Frequency Division of the Physical Measurement Laboratory. 

Because of the high precision in observed data, the reproducible estimation of, say, atomic clock frequency ratios requires careful attention to data processing steps. The idiosyncrasies of processing clock data that we focus on in this report options for the treatment of missing data and for the comparison of low frequency data to high frequency data. We describe the nature of these challenges with supporting examples and we present recommendations for addressing these challenges in a transparent, reproducible manner.      


## Audience

## Collaborators  

## Report Organization 
This note is organized chronologically according to our recommended order of operations. In Section \ref{sec:dataprep}, we discuss necessary data preparation steps, Section \ref{sec:missingdata} covers the handling of missing clock shift data, Section \ref{sec:interp} details techniques for handling multiple, misaligned data sets, and Section \ref{sec:iterate} discusses how to iterate these steps resulting in a coherent final data set for estimation and inference. Section \ref{sec:summary} concludes with a summary and remarks on next steps.   


# Data Preparation 

The data considered in this note include independent time series of clock and comb data. We assume that upon reading the data into Python, all variables are of type float for high precision. The next two subsections however only apply to clock data files. The last subsection applies to all data files. 

Due to the nature of the machinery involved in generating clock and comb data, any time series analyzed will be an irregularly observed one. That is, the interval between consecutive measurement times $t_{i}$ and $t_{i+1}$, where $t_{i}$ is a particular MDJ value, is non-constant. This is true even for comb data, though in a much less drastic way. For example, most intervals for data from the ErYb comb are around $t_{i+1} - t_{i} = 0.000012$ however some observational gaps can be almost double that size with, say, $t_{j+1} - t_{j} = 0.000022$. In any case, these discrepancies in the time at which the frequency data is collected result in an irregularly sampled time series which poses its own challenges in analysis. Typically, such irregularities are corrected by interpolation which is the topic of a later section.  

Data filtering is not a topic of this manuscript but it is important to note as a part of the data pre-processing stage. Scientists, on an *ad hoc* basis, will mark the clock frequency observations as "good" or "bad". This is indicated with a binary variable $IS_{GOOD} = 0, \text{ if data point is not reliable}$ and $IS_{GOOD} = 1,$ otherwise. 


## Exclude Low Quality Clock Data 

Having obtained clock shift files that include the three variables $t_{MJD}$, $f_{shift}$, and $IS_{GOOD}$, a comb data file containing $t_{MJD}$ and any other variables necessary to compute the optical frequency for each clock, and ensuring the values of the variables are high precision decimal type, the first step in processing the data for analysis is to exclude any clock observations where $IS_{GOOD}=0$. 

For example, suppose the $Al+$ clock data is stored in a data frame called *shift_data_Al*. In order to analyze only the data that is marked as reliable by the clock scientist, we can use the following Python code to create a subset containing only the good data, named *shift_data_Al_good*. 

```{python, eval=FALSE, echo=TRUE}
good_condition_al = shift_data_Al["IS_GOOD"] == 1
shift_data_Al_good = shift_data_Al[good_condition_al].reset_index(drop=True)
```


## Visualize Missing Data 

After excluding low quality clock data, the next step is to visualize any missing shift values. Although there may not be any missing data once the previous step has been completed, due to the *ad hoc* nature of data quality marking, we will discuss how to assess any remaining missing data for completeness. 

[python package to visualize with image examples of missing data and complete data] 

[strategy to handle missing data] 
Identify large gaps vs individual or small sequences of missing values of shift data. Visualize gaps in the clock frequency data sets.

One may use interpolation techniques (see next section) for short sequence of individual missing values but not recommended for large gaps. larger gaps should be factored into the next step where one decides which window of observations will be analyzed.  

If large gaps are maintained, caution about how this may impact subsequent steps and conclusions. 

## Determine Overlapping Windows of Observation 

An important characteristic of clock and comb data is that the beginning and end point of observations may vary among each data file under consideration. If there are any large gaps of missing data in the clock shift files, this should also be taken into consideration when determining the start and end time points of the data. Therefore, a necessary component to preparing the data for analysis is first identifying the overlapping windows of observation for each time series. The goal of this step is to find a time interval $[t_{MJD,i}, t_{MDJ, j}]$ that includes the largest possible intersection of observations from each data file. 

Let $X = 60535$ and consider, for example, a time series of comb data that begins at $t_{MJD} = X + 0.682346$ and ends at $t_{MJD} = X + 0.911951$. Suppose we are interested in estimating the average frequency ratio of the $Al+$ and $Sr$ clocks which begin and end at $[X + 0.6818403, X + 0.9108218]$ and $[X + 0.705143550906, X + 0.9791847021]$, respectively. For now, suppose there are no gaps of missing data in either clock time series. 

[include graphic showing intervals]

The follow Python functions are useful in determining the final intersecting time interval used for analysis. 

```{python, eval=FALSE, echo=TRUE}
# Extracts series element that is as close to the target as possible without going over.
def lb_extract(target, data):
    inx = 0
    stopper = 1
    while stopper == 1:
        if data[inx] <= target:
            inx += 1
        else:
            return inx  

# Extracts series element that is as close to target as possible without going under.  
def ub_extract(target, data):
    inx = 1
    stopper = 1
    while stopper == 1:
        if data[len(data)-inx] >= target:
            inx += 1
        else:
            return len(data)-inx 
```


# Interpolation 

## Regular Time Intervals 

For comb data, interpolate to get data that lies on regular time intervals.

Note that this step must be completed for each element necessary to calculate the optical frequency of each clock based on the comb equation. If analyzing three clocks, say, $Yb$, $Sr$, and $Al+$, then in addition to interpolating the values of SDR:frep_ErYb, onto a time grid of equidistant $MDJ$ points, one must also interpolate the values of fb_Al_ErYb and fb_Yb_ErYb. 


## Frequency Alignment

AFTER data is processed, deciding how to handle missing values, THEN address the likely issue of differences in frequency of observations with interpolation techniques. 

Interpolation - to get clock data to match up with comb time intervals, key difference from imputation step is data is contained w/in a certain range of start/stop MJD values 

\begin{itemize}
	\item numpy.interp and pandas.interpolate, linear 
	\item other options: incorporate randomness, kalman smoothing
  \item lit review 
    \begin{itemize}
    \item including (but not limited to) the problem of transforming a low-frequency series into a high-frequency one
      \begin{itemize}
      \item A Survey of Methods to Interpolate, Distribute and  Extrapolate Time Series (2010) https://www.scirp.org/html/3396.html - Kalman filtering Sec. 6 
      \end{itemize}
    \item 
    \end{itemize} 
\end{itemize}


# Multi-taper Spectral Analysis 

Some comments on advantage of this analysis approach in requiring less interpolation and producing more reliable estimates. 


# Summary and Next Steps 

Calculate clock frequencies by adding together comb frequencies and shift data, scaled by the total correction amount 

End results: clock ratio data to compute offset to compare to previous measurements (may contain missing values?) 


<!--- # Relevant TS properties 

- type of missingness - random (MCAR (e.g. sensor failure), MAR, NMAR), structural deficiency

- trend(s) - autocorrelation plot slowly decreases as the lag increases; Cox-Stuart Test

- seasonality - factors of fixed, known frequencies; autocorrelation plot has larger values for the multiples of the seasonal frequency; Autocorrelation Testing of a certain order 

- noise type

  * white - little/no autocorrelation, stationary TS; autocorrelation plot w/ approx $95\%$ of the spikes within an interval of $\pm 1.96/\sqrt{T}$, where $T$ is the length of the time series; Autocorrelation Testing (i.e. the autocorrelation of the signal must be different from zero only for order zero)

  * flicker/pink 
---> 