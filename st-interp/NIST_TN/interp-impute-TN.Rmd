as=hbn---
title: "Clock data interpolation TN draft"
author: "Thornton"
output:
  bookdown::html_document2
bibliography: References.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(MASS)
```


<!--- Open Command Palette: Press F1 or Ctrl + Shift + P to open the command palette.
Run Knit Command: Type rmarkdown: Render and select the desired output format (e.g., HTML, PDF).--->
# Introduction

The purpose of this report is to present a methodological approach for applying interpolation techniques to clock frequency data. This report is a result of collaborative research into statistical methods for the analysis of high-precision atomic clock data by the Statistical Engineering Division of the Information Technology Laboratory and the Time and Frequency Division of the Physical Measurement Laboratory. 

Because it is such high precision data, the reproducible estimation of atomic clock frequency ratios requires careful attention to data processing steps. The idiosyncrasies of processing clock data that we focus on in this report include options for the treatment of missing data and for the comparison of low frequency data to high frequency data. We describe the nature of these challenges with supporting examples and we present recommendations for addressing these challenges in a transparent, reproducible manner. The example analyses and code in this report are written in Python.      

## Data Format

To generate a time series consisting of the ratio (or offset ratio) of two atomic clocks requires the combination of at least four individual time series, two of which are independent of each other and the others. Suppose we are interested in generating a time series for the ratio of an Al+ clock to that of a Yb clock. The two independent series consist of shift data corresponding to each clock. The data files for each clock, Al+ and Yb, contain three variables for each time series, the time stamp (in MJD), the value of the shift (in Hz), and a binary variable, IS_GOOD, denoting whether or not there was a known or suspected issue with the clock readings at that time. The other time series are found in a data file associated with the optical frequency comb. [Are these independent? All output from the same machine?] The columns of the comb data file correspond to frequency readings that can be specific to each clock. Calculating the optical frequencies for each clock based on the ErYb comb, for example, requires time series data corresponding to the three columns labeled fb_Yb_ErYb, fb_Al_ErYb, and SDR:frep_ErYb. [what do these names mean?] The Al+/Yb ratio is then found by adding the clock shift values to the optical frequencies for each clock and then taking the ratio of these sums multiplied by a scalar correction factor. 

## Report Organization 
The next section presents some background information and intoduces an example data set that will be referenced throughout. Section 3 discusses the initial data processing and exploration steps, Section 4 covers interpolation methods for handling multiple, misaligned data sets, and Section 5 concludes with a summary and by reiterating the steps needed to produce a coherent final data set for estimation and inference of a clock ratio time series. 


<!-------------------------------------------------------------->
# Background 

The challenge presented here has several layers that complicate a traditional time series analysis. Although the ultimate analysis is of a single time series, let's call this the target series, the target series is computed based on calculations involving at least three individual time series, we will call these sub-series. Some sub-series may be independent of the others, each may contain missing values, and each is an irregularly sampled time series, possibly at very different frequencies. Furthermore, the sub-series may have slightly or dramatically different start and end times. 

The application that motivates this article is the analysis of atomic clock ratio data. In this context, there are at least four sub-series involved in deriving the target series; two independent clock time series consisting of shift values together with an indicator variable representing the quality of each observation and, for each clock, there is another time-series that is observed in accordance with the readings from an optical frequency comb (OFC). The comb experiment produces several time series, two of which match the two clock types used in the ratio target series, the others of which may be used in the calculation of target series based on experimental settings.  

[Background on modeling clock data @percival - create bib item]

[Background on existing work on irregularly sampled time series.] @shukla2019 @nieto2015 @erdogan2005 @eckner2012

@nieto2015 note that the study of unequally spaced time series can broadly be grouped into several different approaches. The first two dominant approaches are to approach the problem by deriving models for the unequally spaced data as is, or to reduce the irregularly sampled data to equally spaced observations first and then apply standard analysis techniques. The former approach.... Yet another perspective is to view the analysis of unequally spaced time series as a problem of missing data. Methods from this perspective may be evaluated by their ability to predict missing observations [@nieto2015]. 

@pavia2010 provide a thorough review of interpolation techniques with an emphasis on temporal disaggregation and benchmarking. @pavia2010 discuss both a temporal perspective and analysis in the frequency domain. In this article, we restrict our focus to analysis in the time domain. In the example application of atomic clock ratio data, the results of interest include estimates of the mean and Allan deviation of the target series. In addition to providing a useful overview of common time series interpolation techniques, @lepot2017 also remark upon the assessment of uncertaintites that result from interpolation, noting that the law of propagation of uncertaintites does not apply in this case. In Section 4, we focus our attention on interpolation methods that permit uncertainty quantification since this is of interest to our scientific collaborators. 

[More background on existing methods for univariate time series imputation from @howe2021, @pavia2010, @knotters2010, @eckner2012]

[Background on mixed frequency data analysis] 

[Background on existing approaches for combining time series i.e. convolution of time series]



<!-------------------------------------------------------------->
# Data Processing and Exploration

## Data Processing 

There are two main processing steps that must be preformed before the data can be analysed. First, all signal data, including constants for correction factors, must be converted to high precision decimal types in Python. [add sentence or two about this]  
Second, each clock's shift data must be filtered to remove any data deemed poor quality based on the IS_GOOD variable. [add sentence about this and example code chunk]  

## Visualize Missing Data 

Data visualization is an important first step to identify any missing data or unexpected patterns. It is crucial that individual time series are visualized before they are combined. This step also helps with the determination of identifying the overlapping window of observations which is the topic of the next section. 

To understand the magnitude of any missing data issues, one can count the missing observations in each time series. For example, the following Python code reports the total number of missing data points (i.e. truly empty or null data values) in `myseries`. 

```{python, eval=FALSE, echo=TRUE}
print(myseries.isna().sum())
```

Another useful summary is the length of the longest sequence of missing values. The code below calculates the longest sequence (`max_streak`) of missing values in `myseries`.

```{python, eval=FALSE, echo=TRUE}
is_na = myseries.isna()
max_streak = current_streak = 0
for val in is_na:
    if val:
        current_streak += 1
        max_streak = max(max_streak, current_streak)
    else:
        current_streak = 0
print("Longest sequence of NaNs:", max_streak, '\n')
```

While these numerical summaries of missing values are informative, they are not enough to identify any unusual patterns of missingness. For this, there is no substitute for a visual analysis of the data. A useful Python package for visualizing missing data is the package "missingno" which can be implemented on a data frame, `time_series_data`, as follows. 

```{python, eval=FALSE, echo=TRUE}
import missingno as msno

msno.matrix(time_series_data)
plt.show()
```

The treatment of long sequences of missing data is distinct from the treatment of individual missing data points. While common imputation techniques for univariate time series can be implemented for individual missing values, long sequences of missing values indicate a need to reconsider the start and end times for each sub-series. This is the topic of the next subsection. Discussed in more detail later, interpolation techniques may be applied to impute individual missing data values. The only distinction between imputation and interpolation in this context is a matter of perspective as @nieto2015 remark that "unevenly spaced time series have also been treated as equally spaced time series with missing observations."


## Determine Overlapping Window of Observation 

An important characteristic of clock and comb data is that the beginning and end point of observations may vary among sub-series. If there are any large gaps of missing data in the clock shift files, this should also be taken into consideration when determining the start and end time points of the data. [This may not be what to recommend. Need to think about the scope.] Therefore, a necessary component to preparing the data for analysis is first identifying the overlapping windows of observation for each time series. The goal of this step is to find a time interval $[t_{i}, t_{j}]$ that includes the largest possible intersection of non-missing observations from each sub-series. 

Consider the figure below which marks the start and end points of observations for each sub-series, a time series of comb data, a time series from two different atomic clocks. If there are no large gaps of missing values in any of these series, the largest window of observation for the target series (which in this case is a ratio of the two clock values) begins at the first time point of the comb series and ends at the last time point of Clock 1's series. Even though there are more observations outside of this range, these observations cannot be used because values from each time series are necessary to construct the target series.  

```{r, echo=FALSE, fig.cap="Example start and end times relative to one another for two clock time series and for the observations from an optical frequency comb.", fig.lp="fig:start_end_TS"}
library(ggplot2) 
x <- 60535
comb <- c(x + 0.71, x + 0.92)
al <- c(x + 0.6818403, x + 0.9108218 )
sr <- c(x + 0.705143550906, x + 0.9791847021)
df <- data.frame(c("Clock 1", "Clock 2", "OFC"), rbind(al, sr, comb))
colnames(df) <- c("Series", "start", "end")
rownames(df) <- NULL
df_plot <- data.frame(Series = rep(df$Series,2), time = c(df$start, df$end)) 

ggplot(df_plot) +
  geom_point(aes(x=time, y=0, color=Series), size = 15, shape="|", alpha=0.7)  +
  annotate("segment", x=min(df$start)-0.01, xend=max(df$end)+0.01, y=0, yend=0, linewidth=1) +
#  annotate("segment", x=min(df$start)-0.01, xend=min(df$start)-0.01, y=-0.0001, yend=0.0001, linewidth=2) +
#  annotate("segment", x=max(df$end)+0.01, xend=max(df$end)+0.01, y=-0.0001, yend=0.0001, linewidth=2) +
#  geom_text(aes(label = x), col="white") +
  scale_x_continuous(limits = c(min(df$start)-0.01, max(df$end)+0.01)) +
  scale_y_continuous(limits = c(-0.001,0.001)) +
#  scale_color_manual(values = unname(colours)) + 
  theme(panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())
```

For this reason, the target time series cannot be computed over any time intervals containing sequentially missing values for one or more of the sub-series. It is more accurate in such cases to consider data before and after a large gap as separate time series with smaller sample sizes. Thus, in the case where a sub-series contains at least one large gap of missing values, the final overlapping window of observation will be considerably smaller as will the target series sample size. [Reiterate note about how keeping missing values until the end is not a valid approach - even if it can be computed with software.]   

In addition to visualizing the sub-series, the follow Python functions may be useful for detecting the overlapping window of observation for the target series. 

```{python, eval=FALSE, echo=TRUE}
# Extracts series element that is as close to the target as possible without going over.
def lb_extract(target, data):
    inx = 0
    stopper = 1
    while stopper == 1:
        if data[inx] <= target:
            inx += 1
        else:
            return inx  

# Extracts series element that is as close to target as possible without going under.  
def ub_extract(target, data):
    inx = 1
    stopper = 1
    while stopper == 1:
        if data[len(data)-inx] >= target:
            inx += 1
        else:
            return len(data)-inx 
```


<!-------------------------------------------------------------->
# Interpolation 

## Frequency Alignment

The culmination of the previous data processing steps yields several complete sub-series (i.e. that have no missing values) that begin and end at the same time points but that may be observed with very different, and irregular, frequencies. Because each series is comprised of observations along a distinct, irregular time scale, we are not yet ready to compute the target series. In the context of atomic clock data, the comb time series are typically observed more regularly than any individual clock shift time series. Thus the comb data represent a high-frequency sub-series and the clock shift data are low-frequency sub-series in comparison. Furthermore, it may be the case that each clock sub-series is sampled at very different rates. 

Without loss of generality and for the sake of illustration, let us suppose that we are constructing a target series from an optical frequency comb sub-series and two clock sub-series. Suppose further that Clock 1 is observed with either a similar or a lower frequency than Clock 2 and that Clock 2 is observed with a similar frequency as the comb sub-series. Symbolically, we are assuming $n_{C1} \leq n_{C2} \approx n_{OFC}$. In this context, the target series is a ratio time series for any two clocks. To derive the target series, the time points of observations for each sub-series must be in alignment with one another. That is, the times of observation for each series, $t_{C1,i} = t_{C2,i} = t_{OFC,i}$ for $i = 1, \dots, n$. Note that this alignment condition can be met even if the series are irregularly sampled; irregular sampling for, say, Clock 1 would simply mean that $(t_{C1, i+1} - t_{C1, i})$ is not equal for all $i \in \{2, \dots, n_{C1}\}$. Here we give an overview of several methodological approaches one may take to align each series.

**Method 1**: Align Clock 1 with the observations of the comb. Then align Clock 2 with the observations of the comb. Proceed to derive the ratio time series for analysis. The result of this approach will be a time series with the same irregularities in sampling as the comb series.  

<!---**Method 2**: Align Clock 1 with the observations of Clock 2. Then align the new series for Clock 1 with the comb. Finally, align Clock 2 with the comb and proceed to derive the ratio time series for analysis. This approach may be reasonable if Clock 1 is observed with a much lower frequency compared to Clock 2. As with Method 1, the result will be a time series with the same irregular observation pattern as the comb data. --->

**Method 2**: Realign the comb time series so that the observations are regularly sampled along the time interval of interest, i.e so that $(t_{OFC, i+1} - t_{OFC,i})$ are equal for all $i \in \{2, \dots, n_{OFC}\}$. Align Clock 1 with the new comb series. Align Clock 2 with the new comb series. Proceed to derive the ratio time series for analysis. The result of this approach will be a regularly sampled time series.  

**Method 3**: Align Clock 1 with the observations of Clock 2. Then align the comb with Clock 2. Proceed to derive the ratio time series for analysis. The result of this approach will be a target series with the same irregularities in sampling as in Clock 2. This approach essentially disregards (or masks) comb data for the sake of matching the observational time points of Clock 2. 

**Method 4**: Realign the time series from Clock 2 so that the observations are regularly sampled along the time interval of interest, i.e so that $(t_{C2, i+1} - t_{C2,i})$ are equal for all $i \in \{2, \dots, n_{C2}\}$. Align Clock 1 with the new series for Clock 2. Align the comb series with the new series for Clock 2. Proceed to derive the ratio time series for analysis. The result of this approach will be a regularly sampled time series. 

Note that none of the methods listed above suggest realigning a high frequency series to match a low frequency series (aggregating). This is because such an approach would sacrifice valuable information contained in the higher frequency series without serving a practical purpose in the context of our example. <!---and thus is generally not advisable. Within the context of this example where---> When the target series is a ratio of data from two separate clocks, it is worth considering how variable the values in the comb sub-series are. If these values are relatively constant, with practically negligible variability in the values associated with each clock, then Methods 3 and 4 may be preferable to Methods 1 and 2 because masking the values from the comb sub-series will be of little to no consequence. 
<!---Although it is listed above for the sake of completeness, Method 2 is inferior to the other methods because it results in an irregularly sampled time series and it requires three separate interpolation steps. In comparison,--> 
Methods 1 and 3 only require two separate interpolation steps. The drawback of these two methods, however is that the resulting time series is still irregularly observed. "A comparative analysis of time series is not feasible if the observation times are different @nieto2015." "Any attempt of comparative or associative analysis between... two time series requires them to have both been measured at the same times @nieto2015." However, if the sampling irregularities of the target series are so small that that are practically meaningless, then these approaches are preferable to the others because they require fewer interpolation steps. 

Methods 2 and 4, on the other hand, each require three separate interpolation steps. The advantage of these methods is that they result in a regularly sampled target series. Although it is possible (and perhaps preferable) to coherently analyse an irregularly sampled time series in the time domain (see e.g. @eckner2012), this is not an approach that is in popular use. @eckner2019 has developed software to implement an analysis on irregularly sampled time series in the C library "utsAlgorithms" and the R package "utsOperators". Example implementations for each of these four methods are included in next subsection.    
<!---Depending on the type of analysis, having an irregularly sampled time series may or may not be of concern. For example, if the analysis is based upon a multitaper spectral approach such as that in [cite clock paper], then the sampling irregularities do not present an issue. [cite sources as to why not]. However, if the analysis is to proceed in [the time domain?], irregularities in the observational frequency of the time series will bias common estimates of interest such as the mean and AVAR. [cite sources]---> 





## Interpolation techniques 

Once a method for alignment is determined, the implementation of the alignment will occur through time series interpolation. There are many different techniques for interpolating time series data. [Cite some sources.]  


@pavia2010 section 6 on kalman filter method (others don't seem applicable) 

@lepot2017 categorize two general interpolation approaches, deterministic and stochastic. Popular deterministic approaches include nearest-neighbor interpolation, polynomial interpolation, interpolation through distance weighting, and interpolation through signal analysis. Stochastic interpolation techniques include regression methods, autoregressive methods, machine learning methods, methods based on data dynamics and methods based on kriging. Although extensive, these lists of approaches are far from exhaustive as this is an active area of statistical research and interpolation is a highly customizable proceedure. "The ranking of desirable \[interpolation\] methods (obtained through a trade-off of criteria) could be strongly dependent on the size of the gaps, and the nature of recorded phenomena and data [@lepot2017]."
    

@knotters2010 provide another useful summary of a collection of interpolation techniques, specifically for use in environmental research. In particular, they remark upon the quantification of the accuracy of interpolated values and the incorporation of process knowledge (among other factors) which are relevant in the context of clock data. Importantly, they highlight that validation studies are necessary to assess the quality of interpolation studies. 

For interpolation in time, and for methods that quantify uncertainty without depending on ancillary information, @knotters2010 discuss methods based on kriging, ARIMA modelling, a state-space approach and Kalman filtering, and minimum mean absolute error linear interpolation (MMAELI). Model based interpolation approaches (e.g. ARIMA for transfer function-noise models) are appropriate for time series that are regularly observed. Kalman filterning or smoothing however, can be applied to interpolate irregularly observed time series [@knotters2010]. Although there are many more interpolation techniques covered in @knotters2010, for the application to clock ratio data, we are not concerned with interpolation in space or space-time nor are we concerned with methods that do not quantify uncertainty about the interpolated values. Furthermore, we do not suppose that ancillary information is available. (see section 3.1.3 and 3.2 as well) 

aggregation (or upscaling) is the process of transferring information from a more detailed scale to a coarser scale.  and disaggregation (downscaling) time series (section 6); software in section 7

@nieto2015  


[mention and demonstrate use of python packages] 

For the convolution of univariate time series... 

**Example implementation of Method 1.** We will compare two analysese of a clock ratio time series in this example. The first analysis (a) ignores the irregular sampling of the time series. The second analysis (b) makes use of the R package "utsOperators" to derive estimators that take into account the irregularies in sampling. 

**Example implementation of Method 2.**

**Example implementation of Method 3.** We will compare two analysese of a clock ratio time series in this example. The first analysis (a) ignores the irregular sampling of the time series. The second analysis (b) makes use of the R package "utsOperators" to derive estimators that take into account the irregularies in sampling. 

**Example implementation of Method 4.**



<!-------------------------------------------------------------->
# Conclusion 

[Summarize the results of the examples in the previous section] 


The process we have outlined for aligning the observational frequency of clock time series follows these steps: 

Step 1) Process the data for each time series to determine the final window of observation and exclude low-quality data. 

Step 2) Determine if there are any missing data and decide whether or not to impute values that are missing. 

Step 3) Determine which method of frequency alignment will be used and decide upon an interpolation technique to implement the alignment. 

Calculate clock frequencies by adding together comb frequencies and shift data, scaled by the total correction amount 

[all of this analysis is occurring within the time domain? not the frequency domain?] 



