{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87898ba3",
   "metadata": {},
   "source": [
    "**Import and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ee5d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## comb data \n",
    "def open_ErYb_data(data_path, header=2):\n",
    "    # keys to read out as string\n",
    "    key2read = [\"MJD\", \"timer\", \"SDR:frep_ErYb\", \"fo_ErYb\", \"fb_Si_ErYb\", \"fb_Al_ErYb\", \"fb_Yb_ErYb\"] \n",
    "    types = {key: str for key in key2read}\n",
    "    types[\"MJD\"] = float\n",
    " \n",
    "    # # Read the CSV file\n",
    "    data = pd.read_csv(data_path, header=1, delimiter=\"\\t\", dtype=types, engine=\"python\")\n",
    " \n",
    "    # Convert the strings to Decimal for the given keys\n",
    "    for k in key2read:\n",
    "        data[k] = data[k].apply(Decimal)\n",
    " \n",
    "    # reindex data\n",
    "    data.index = range(len(data))\n",
    " \n",
    "    return data[list(types.keys())]\n",
    "\n",
    "## Al shift data \n",
    "def open_shiftfile_Al(datapath):\n",
    "    data = pd.read_csv(datapath, header=30, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    # Replace column names\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    # Change column type from float to bool\n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    # Put NaN in data[\"shift\"] where data[\"IS_GOOD\"] is 0\n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    # Change column type to float\n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    " ## Yb shift data\n",
    "def open_shiftfile_Yb(datapath):\n",
    "    data = pd.read_csv(datapath, header=8, delimiter=r\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    # Replace column names\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    # Change column type from float to bool\n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    # Put NaN in data[\"shift\"] where data[\"IS_GOOD\"] is 0\n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    # Change column type to float\n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    " \n",
    " \n",
    "################################################################################\n",
    "#############  Functions to find optical frequencies with comb equation ########\n",
    "################################################################################\n",
    " \n",
    "# frequency for Al+ clock\n",
    "def compute_nuAl_ErYb(data):\n",
    "    data[\"nuAl\"] = -Decimal(\"105e6\") + Decimal(\"560444\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Al_ErYb\"]\n",
    "    data[\"nuAl\"] = Decimal(4) * data[\"nuAl\"]   \n",
    "\n",
    "# freuency for Yb clock\n",
    "def compute_nuYb_ErYb(data):\n",
    "    data[\"nuYb\"] = -Decimal(\"105e6\") + Decimal(\"518237\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Yb_ErYb\"]\n",
    "    data[\"nuYb\"] = Decimal(2) * data[\"nuYb\"]\n",
    "\n",
    "################################################################################\n",
    "#############################  Load data #######################################\n",
    "################################################################################\n",
    " \n",
    "path = \"/Users/smt3/Documents/GitHub/atomic-clock/st-interp/three_clocks/\"\n",
    "\n",
    "# load comb data\n",
    "data_ErYb = open_ErYb_data(path + \"20240813_Deglitched_ErYb_only1.dat\")\n",
    " \n",
    "# load Al shift data \n",
    "shift_data_Al = open_shiftfile_Al(path + \"20240813_Al+_Freq_Shifts_ErYb.dat\")\n",
    "\n",
    "# load Yb shift data\n",
    "shift_data_Yb = open_shiftfile_Yb(path + \"20240813_Yb_Freq_Shifts.txt\")\n",
    "\n",
    "################################################################################\n",
    "###############  get optical frequencies #############################\n",
    "################################################################################\n",
    " \n",
    "compute_nuAl_ErYb(data_ErYb)\n",
    "compute_nuYb_ErYb(data_ErYb) \n",
    " \n",
    "################################################################################\n",
    "#########################  Data Processing #####################################\n",
    "################################################################################\n",
    "\n",
    "## Extract only \"IS_GOOD\" data for analysis \n",
    "good_condition_al = shift_data_Al[\"IS_GOOD\"] == 1\n",
    "shift_data_Al_good = shift_data_Al[good_condition_al].reset_index(drop=True)\n",
    "good_condition_yb = shift_data_Yb[\"IS_GOOD\"] == 1\n",
    "shift_data_Yb_good = shift_data_Yb[good_condition_yb].reset_index(drop=True)\n",
    "\n",
    "common_mjd = data_ErYb[\"MJD\"].astype(float)\n",
    "nuAl = data_ErYb[\"nuAl\"].astype(float)\n",
    "nuYb = data_ErYb['nuYb'].astype(float)\n",
    "\n",
    "print(\"Missing Al comb nu: \", nuAl.isna().sum())\n",
    "print(\"Missing Yb comb nu: \", nuYb.isna().sum())\n",
    "\n",
    "#is_na = nuAl.isna()\n",
    "is_na = nuYb.isna()\n",
    "max_streak = current_streak = 0\n",
    "for val in is_na:\n",
    "    if val:\n",
    "        current_streak += 1\n",
    "        max_streak = max(max_streak, current_streak)\n",
    "    else:\n",
    "        current_streak = 0\n",
    "print(\"Total NaNs:\", is_na.sum())\n",
    "print(\"Longest sequence of NaNs:\", max_streak, '\\n')\n",
    "#Note: nuYb ends before nuAl (though have matching MJD values)\n",
    "\n",
    "len_comb = len(common_mjd) \n",
    "len_Al = len(shift_data_Al_good['shift'])             \n",
    "len_yb = len(shift_data_Yb_good['shift'])\n",
    "\n",
    "end_comb_inx = len_comb - 1 - max_streak\n",
    "print(\"nuAl and nuYb start and end MJD: [\", common_mjd[0], ', ', common_mjd[end_comb_inx], ']')\n",
    "print(\"Al good shift start  and end MJD: [\", shift_data_Al_good['MJD'][0], ', ', shift_data_Al_good['MJD'][len_Al-1], ']')\n",
    "print(\"Yb good shift start and end MJD: [\", shift_data_Yb_good['MJD'][0], ', ', shift_data_Yb_good['MJD'][len_yb-1], ']')\n",
    "\n",
    "last_start_time = shift_data_Yb_good['MJD'][0]\n",
    "first_end_time = shift_data_Yb_good['MJD'][len_yb-1]\n",
    "\n",
    "#function to extract element as close to target as possible w/out going over\n",
    "def lb_extract(target, data):\n",
    "    inx = 0\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data[inx] <= target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return inx  \n",
    "\n",
    "#function to extract element as close to target as possible w/out going under \n",
    "def ub_extract(target, data):\n",
    "    inx = 1\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data[len(data)-inx] >= target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return len(data)-inx  \n",
    "\n",
    "\n",
    "comb = pd.DataFrame()\n",
    "comb_start = ub_extract(target = last_start_time, data = common_mjd)  \n",
    "comb_end = lb_extract(target = first_end_time, data = common_mjd)  \n",
    "comb[\"MJD\"] = common_mjd[comb_start:comb_end] \n",
    "comb[\"nuAl\"] = nuAl[comb_start:comb_end]\n",
    "comb['nuAl'] = pd.to_numeric(comb['nuAl'], errors='coerce')\n",
    "comb[\"nuYb\"] = nuYb[comb_start:comb_end]\n",
    "comb['nuYb'] = pd.to_numeric(comb['nuYb'], errors='coerce')\n",
    "\n",
    "al_start = ub_extract(target = last_start_time, data = shift_data_Al_good[\"MJD\"])\n",
    "al_end = lb_extract(target = first_end_time, data = shift_data_Al_good[\"MJD\"])  \n",
    "shift_data_Al_high = shift_data_Al_good[al_start:al_end]\n",
    "\n",
    "shift_data_Yb = shift_data_Yb_good\n",
    "\n",
    "## reset df indicies for next step \n",
    "comb = comb.reset_index(drop=True)  \n",
    "shift_data_Al_high = shift_data_Al_high.reset_index(drop=True)\n",
    "shift_data_Yb = shift_data_Yb.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672f9ce",
   "metadata": {},
   "source": [
    "**Method 1**: Align Clock 1 with the observations of the comb. Then align Clock 2 with the observations of the comb. Proceed to derive the ratio time series for analysis. The result of this approach will be a time series with the same irregularities in sampling as the comb series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a1258",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8e881d3",
   "metadata": {},
   "source": [
    "**Method 2**: Realign the comb time series so that the observations are regularly sampled along the time interval of interest, i.e so that $(t_{OFC, i+1} - t_{OFC,i})$ are equal for all $i \\in \\{2, \\dots, n_{OFC}\\}$. Align Clock 1 with the new comb series. Align Clock 2 with the new comb series. Proceed to derive the ratio time series for analysis. The result of this approach will be a regularly sampled time series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4978a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a35720",
   "metadata": {},
   "source": [
    "**Method 3**: Align Clock 1 with the observations of Clock 2. Then align the comb with Clock 2. Proceed to derive the ratio time series for analysis. The result of this approach will be a target series with the same irregularities in sampling as in Clock 2. This approach essentially disregards (or masks) comb data for the sake of matching the observational time points of Clock 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862721b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd36efac",
   "metadata": {},
   "source": [
    "**Method 4**: Realign the time series from Clock 2 so that the observations are regularly sampled along the time interval of interest, i.e so that $(t_{C2, i+1} - t_{C2,i})$ are equal for all $i \\in \\{2, \\dots, n_{C2}\\}$. Align Clock 1 with the new series for Clock 2. Align the comb series with the new series for Clock 2. Proceed to derive the ratio time series for analysis. The result of this approach will be a regularly sampled time series. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252b91b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
