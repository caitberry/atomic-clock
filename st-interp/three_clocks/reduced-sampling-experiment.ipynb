{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "################################################################################\n",
    "#############  Functions for data loading #####################################\n",
    "################################################################################\n",
    "\n",
    "## comb data \n",
    "def open_ErYb_data(data_path, header=2):\n",
    "    # keys to read out as string\n",
    "    key2read = [\"MJD\", \"timer\", \"SDR:frep_ErYb\", \"fo_ErYb\", \"fb_Si_ErYb\", \"fb_Al_ErYb\", \"fb_Yb_ErYb\"] \n",
    "    types = {key: str for key in key2read}\n",
    "    types[\"MJD\"] = float\n",
    " \n",
    "    # # Read the CSV file\n",
    "    data = pd.read_csv(data_path, header=1, delimiter=\"\\t\", dtype=types, engine=\"python\")\n",
    " \n",
    "    # Convert the strings to Decimal for the given keys\n",
    "    for k in key2read:\n",
    "        data[k] = data[k].apply(Decimal)\n",
    " \n",
    "    # reindex data\n",
    "    data.index = range(len(data))\n",
    " \n",
    "    return data[list(types.keys())]\n",
    "\n",
    "## Al shift data \n",
    "def open_shiftfile_Al(datapath):\n",
    "    data = pd.read_csv(datapath, header=30, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    # Replace column names\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    # Change column type from float to bool\n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    # Put NaN in data[\"shift\"] where data[\"IS_GOOD\"] is 0\n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    # Change column type to float\n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    "## Sr shift data \n",
    "def open_shiftfile_Sr(datapath):\n",
    "    data = pd.read_csv(datapath, header=22, delimiter=\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    # Replace column names\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    # Change column type from float to bool\n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    # Put NaN in data[\"shift\"] where data[\"IS_GOOD\"] is 0\n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    # Change column type to float\n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    " ## Yb shift data\n",
    "def open_shiftfile_Yb(datapath):\n",
    "    data = pd.read_csv(datapath, header=8, delimiter=r\"\\t\", dtype={1: str}, engine=\"python\")\n",
    " \n",
    "    # Replace column names\n",
    "    data.columns = [\"MJD\", \"shift\", \"IS_GOOD\"]\n",
    " \n",
    "    # Change column type from float to bool\n",
    "    data[\"IS_GOOD\"] = data[\"IS_GOOD\"].apply(lambda x: x == 1.0)\n",
    " \n",
    "    # Put NaN in data[\"shift\"] where data[\"IS_GOOD\"] is 0\n",
    "    data.loc[~data[\"IS_GOOD\"], \"shift\"] = np.nan\n",
    " \n",
    "    # Change column type to float\n",
    "    data[\"shift\"] = data[\"shift\"].apply(float)\n",
    " \n",
    "    return data\n",
    " \n",
    "################################################################################\n",
    "#############  Functions to find optical frequencies with comb equation ########\n",
    "################################################################################\n",
    " \n",
    "# frequency for Al+ clock\n",
    "def compute_nuAl_ErYb(data):\n",
    "    data[\"nuAl\"] = -Decimal(\"105e6\") + Decimal(\"560444\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Al_ErYb\"]\n",
    "    data[\"nuAl\"] = Decimal(4) * data[\"nuAl\"]   \n",
    "\n",
    "# frequency for Sr clock \n",
    "def compute_nuSr_ErYb(data):\n",
    "    data[\"nuSi\"] = -Decimal(\"105e6\") + Decimal(\"388752\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - Decimal(\"100e6\")\n",
    "    data[\"nuSr\"] = (Decimal(\"1716882\") / Decimal(\"777577\")) * (data[\"nuSi\"] - Decimal(\"216e6\"))\n",
    "\n",
    "\n",
    "# freuency for Yb clock\n",
    "def compute_nuYb_ErYb(data):\n",
    "    data[\"nuYb\"] = -Decimal(\"105e6\") + Decimal(\"518237\") * (Decimal(\"1e9\") + data[\"SDR:frep_ErYb\"]) / Decimal(2) - data[\"fb_Yb_ErYb\"]\n",
    "    data[\"nuYb\"] = Decimal(2) * data[\"nuYb\"]\n",
    " \n",
    " \n",
    "################################################################################\n",
    "#############################  Load data #######################################\n",
    "################################################################################\n",
    " \n",
    "path = \"/Users/smt3/Documents/GitHub/atomic-clock/st-interp/three_clocks/\"\n",
    "\n",
    "# load comb data\n",
    "data_ErYb = open_ErYb_data(path + \"20240813_Deglitched_ErYb_only1.dat\")\n",
    " \n",
    "# load Al shift data \n",
    "shift_data_Al = open_shiftfile_Al(path + \"20240813_Al+_Freq_Shifts_ErYb.dat\")\n",
    "\n",
    "# load Sr shift data\n",
    "shift_data_Sr = open_shiftfile_Sr(path + \"20240813_Sr_Freq_Shifts.dat\")\n",
    " \n",
    "# load Yb shift data\n",
    "shift_data_Yb = open_shiftfile_Yb(path + \"20240813_Yb_Freq_Shifts.txt\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "################################################################################\n",
    "###############  get optical frequencies #############################\n",
    "################################################################################\n",
    " \n",
    "compute_nuSr_ErYb(data_ErYb)\n",
    "compute_nuYb_ErYb(data_ErYb)\n",
    "compute_nuAl_ErYb(data_ErYb)\n",
    " \n",
    " \n",
    "\n",
    "################################################################################\n",
    "#########################  Data Processing #####################################\n",
    "################################################################################\n",
    "\n",
    "## Extract only \"IS_GOOD\" data for analysis------------------\n",
    "good_condition_al = shift_data_Al[\"IS_GOOD\"] == 1\n",
    "shift_data_Al_good = shift_data_Al[good_condition_al].reset_index(drop=True)\n",
    "good_condition_sr = shift_data_Sr[\"IS_GOOD\"] == 1\n",
    "shift_data_Sr_good = shift_data_Sr[good_condition_sr].reset_index(drop=True)\n",
    "good_condition_yb = shift_data_Yb[\"IS_GOOD\"] == 1\n",
    "shift_data_Yb_good = shift_data_Yb[good_condition_yb].reset_index(drop=True)\n",
    "\n",
    "\n",
    "## Find common MDJ values--------------------------------\n",
    "#NOTE: the following does not take into account adjustements made based on large chunks of missing data\n",
    "\n",
    "#Change comb mjd type to float  \n",
    "common_mjd = data_ErYb[\"MJD\"].astype(float)\n",
    "nuAl = data_ErYb[\"nuAl\"].astype(float)\n",
    "nuYb = data_ErYb[\"nuYb\"].astype(float)\n",
    "nuSr = data_ErYb[\"nuSr\"].astype(float)\n",
    "#convert comb nu values to high precision decimal \n",
    "nuAl = [Decimal(i) for i in nuAl]\n",
    "nuSr = [Decimal(i) for i in nuSr]\n",
    "nuYb = [Decimal(i) for i in nuYb] \n",
    "\n",
    "# Length of the 'MJD' column\n",
    "len_comb = len(common_mjd) \n",
    "len_Al = len(shift_data_Al_good['MJD'])                  \n",
    "len_Sr = len(shift_data_Sr_good['MJD'])\n",
    "len_Yb = len(shift_data_Yb_good['MJD'])\n",
    "\n",
    "#function to extract element as close to target as possible w/out going over\n",
    "def lb_extract(target, data):\n",
    "    inx = 0\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data[inx] <= target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return inx  \n",
    "\n",
    "#function to extract element as close to target as possible w/out going under \n",
    "def ub_extract(target, data):\n",
    "    inx = 1\n",
    "    stopper = 1\n",
    "    while stopper == 1:\n",
    "        if data[len(data)-inx] >= target:\n",
    "            inx += 1\n",
    "        else:\n",
    "            return len(data)-inx  \n",
    "\n",
    "#Compare start and end points (assuming no missing data b/c IS_GOOD variable already accounted for)\n",
    "print(\"first comb time point: \", common_mjd[0])\n",
    "print(\"first good Al time point: \", shift_data_Al_good[\"MJD\"][0])\n",
    "print(\"first good Sr time point: \", shift_data_Sr_good[\"MJD\"][0])\n",
    "print(\"first good Yb time point: \", shift_data_Yb_good[\"MJD\"][0])\n",
    "start_times = {common_mjd[0], shift_data_Al_good[\"MJD\"][0], shift_data_Sr_good[\"MJD\"][0], shift_data_Yb_good[\"MJD\"][0]}\n",
    "print(\"----->Latest start time: \", max(start_times)) \n",
    "print(\"last comb time point: \", common_mjd[len_comb-1])\n",
    "print(\"last good Al time point: \", shift_data_Al_good[\"MJD\"][len_Al-1])\n",
    "print(\"last good Sr time point: \", shift_data_Sr_good[\"MJD\"][len_Sr-1])\n",
    "print(\"last good Yb time point: \", shift_data_Yb_good[\"MJD\"][len_Yb-1])\n",
    "end_times = {common_mjd[len_comb-1], shift_data_Al_good[\"MJD\"][len_Al-1], shift_data_Sr_good[\"MJD\"][len_Sr-1], shift_data_Yb_good[\"MJD\"][len_Yb-1]}\n",
    "print(\"----->Earliest end time: \", min(end_times), \"\\n\")\n",
    "\n",
    "\n",
    "#NOTE: The following must be edited based on the resuts of the print statements above\n",
    "print(\"since Sr starts the latest, start comb observations at index \", lb_extract(target = shift_data_Sr_good[\"MJD\"][0], data = common_mjd), \n",
    "         \", start Al observations at index \", lb_extract(target = shift_data_Sr_good[\"MJD\"][0], data = shift_data_Al_good[\"MJD\"]),\n",
    "         \" and start Yb observations at index \", lb_extract(target = shift_data_Sr_good[\"MJD\"][0], data = shift_data_Yb_good[\"MJD\"]))\n",
    "print(\"since Yb ends first, end comb observations at index \", ub_extract(target = shift_data_Yb_good[\"MJD\"][len_Yb-1], data = common_mjd), \n",
    "         \", end Sr observations at index \", ub_extract(target = shift_data_Yb_good[\"MJD\"][len_Yb-1], data = shift_data_Sr_good[\"MJD\"]),\n",
    "         \" and end Al observations at index \", ub_extract(target = shift_data_Yb_good[\"MJD\"][len_Yb-1], data = shift_data_Al_good[\"MJD\"]), \"\\n\")\n",
    "\n",
    "#comb MJD index \n",
    "comb = pd.DataFrame()\n",
    "comb_init = lb_extract(target = shift_data_Sr_good[\"MJD\"][0], data = common_mjd) #edit\n",
    "comb_end = ub_extract(target = shift_data_Yb_good[\"MJD\"][len_Yb-1], data = common_mjd) #edit\n",
    "comb[\"MJD\"] = common_mjd[comb_init:comb_end]\n",
    "comb[\"nuAl\"] = nuAl[comb_init:comb_end]\n",
    "comb[\"nuYb\"] = nuYb[comb_init:comb_end]\n",
    "comb[\"nuSr\"] = nuSr[comb_init:comb_end]\n",
    "\n",
    "#Al MJD index \n",
    "shift_data_Al = shift_data_Al_good[lb_extract(target = shift_data_Sr_good[\"MJD\"][0], data = shift_data_Al_good[\"MJD\"]):ub_extract(target = shift_data_Yb_good[\"MJD\"][len_Yb-1], data = shift_data_Al_good[\"MJD\"])]\n",
    "#Sr MJD index \n",
    "shift_data_Sr = shift_data_Sr_good[0:ub_extract(target = shift_data_Yb_good[\"MJD\"][len_Yb-1], data = shift_data_Sr_good[\"MJD\"])]\n",
    "#Yb MJD index  \n",
    "shift_data_Yb = shift_data_Yb_good[lb_extract(target = shift_data_Sr_good[\"MJD\"][0], data = shift_data_Yb_good[\"MJD\"]):len_Yb-1]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## View frequency of MDJ observations in comb \n",
    "comb[\"MJD_diff\"] = comb.MJD.diff()\n",
    "#print(comb.columns)\n",
    "series_MJD_comb_diff = comb[\"MJD_diff\"]\n",
    "print(series_MJD_comb_diff.dtype)\n",
    "#series_MJD_comb_diff.describe()\n",
    "unique_vals = series_MJD_comb_diff.unique()\n",
    "num_unique = len(unique_vals)\n",
    "#print(num_unique)\n",
    "#print(unique_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#########################  Interpolation techniques  ###########################\n",
    "################################################################################\n",
    "# Method 1) Use numpy.interp to interpolate Al, Sr, and Yb shift values onto \n",
    "#           comb MJD values \n",
    "shiftAl1 = np.interp(comb[\"MJD\"], shift_data_Al[\"MJD\"], shift_data_Al[\"shift\"])\n",
    "shiftSr1 = np.interp(comb[\"MJD\"], shift_data_Sr[\"MJD\"], shift_data_Sr[\"shift\"])\n",
    "shiftYb1 = np.interp(comb[\"MJD\"], shift_data_Yb[\"MJD\"], shift_data_Yb[\"shift\"])\n",
    "\n",
    "# change Method 1 data type to high-precision Decimal\n",
    "shiftAl1 = [Decimal(i) for i in shiftAl1]\n",
    "shiftSr1 = [Decimal(i) for i in shiftSr1]\n",
    "shiftYb1 = [Decimal(i) for i in shiftYb1]\n",
    "\n",
    "################################################################################\n",
    "#############################  Plotting  #######################################\n",
    "################################################################################\n",
    " \n",
    "# Ratios from 2020\n",
    "YbSrRatio2020 = Decimal(\"1.2075070393433378482\") \n",
    "AlYbRatio2020 = Decimal(\"2.162887127516663703\")\n",
    "AlSrRatio2020 = Decimal(\"2.611701431781463025\")\n",
    " \n",
    "# frequency corrections\n",
    "masercorrection = Decimal(\"-7.36631e-12\")\n",
    "GR_shift_Al = Decimal(\"-8.114e-16\")\n",
    "GR_shift_Yb = Decimal(\"-8.109e-16\")\n",
    "GR_shift_Sr = Decimal(\"10.660e-16\")\n",
    "GR_shift_sea_level = Decimal(\"-1798.501e-16\")\n",
    "\n",
    "total_correction_Yb = Decimal(\"1\") + GR_shift_Yb + GR_shift_sea_level + masercorrection\n",
    "total_correction_Sr = Decimal(\"1\") + GR_shift_Sr + GR_shift_sea_level + masercorrection\n",
    "total_correction_Al = Decimal(\"1\") + GR_shift_Al + GR_shift_sea_level + masercorrection\n",
    "\n",
    "## Method 1)\n",
    "# add comb frequencies and clock shift files\n",
    "frequency_Yb_ErYb1 = [(i + j) * total_correction_Yb for i,j in zip(comb[\"nuYb\"], shiftYb1)]\n",
    "frequency_Sr_ErYb1 = [(i + j) * total_correction_Sr for i,j in zip(comb[\"nuSr\"], shiftSr1)]\n",
    "frequency_Al_ErYb1 = [(i + j) * total_correction_Al for i,j in zip(comb[\"nuAl\"], shiftAl1)]\n",
    " \n",
    "\n",
    "# Yb/Sr ratio offset from BACON paper\n",
    "frequency_ratio_ErYb1_1 = [(i / j - YbSrRatio2020)/YbSrRatio2020 for i,j in zip(frequency_Yb_ErYb1, frequency_Sr_ErYb1)]\n",
    "print(\"Yb/Sr ratio offset from BACON paper (Method 1)\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb1_1)) )\n",
    " \n",
    "plt.figure()\n",
    "plt.plot(comb[\"MJD\"], frequency_ratio_ErYb1_1, '.')\n",
    "plt.grid(visible=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(visible=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"offset from Yb/Sr ratio BACON ratio\")\n",
    "plt.title(\"Yb/Sr ratio from 8/13/24 on common MJD grid\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Al/Yb ratio offset \n",
    "frequency_ratio_ErYb2_1 = [(i / j - AlYbRatio2020)/AlYbRatio2020 for i,j in zip(frequency_Al_ErYb1, frequency_Yb_ErYb1)]\n",
    "print(\"Al/Yb ratio offset from BACON paper (Method 1)\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb2_1)) )\n",
    " \n",
    "plt.figure()\n",
    "plt.plot(comb[\"MJD\"], frequency_ratio_ErYb2_1, '.')\n",
    "plt.grid(visible=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(visible=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"offset from Al/Yb ratio BACON ratio\")\n",
    "plt.title(\"Al/Yb ratio from 8/13/24 on common MJD grid\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Al/Sr ratio offset  \n",
    "frequency_ratio_ErYb3_1 = [(i / j - AlSrRatio2020)/AlSrRatio2020 for i,j in zip(frequency_Al_ErYb1, frequency_Sr_ErYb1)]\n",
    "print(\"Al/Sr ratio offset from BACON paper (Method 1)\", '{:0.5}'.format(np.nanmean(frequency_ratio_ErYb3_1)) )\n",
    " \n",
    "plt.figure()\n",
    "plt.plot(comb[\"MJD\"], frequency_ratio_ErYb3_1, '.')\n",
    "plt.grid(visible=True, which='major', color='#666666', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(visible=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.xlabel(\"MJD\")\n",
    "plt.ylabel(\"offset from Al/Sr ratio BACON ratio\")\n",
    "plt.title(\"Al/Sr ratio from 8/13/24 on common MJD grid\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
